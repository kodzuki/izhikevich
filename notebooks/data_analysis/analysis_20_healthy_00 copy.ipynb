{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0220a4e",
   "metadata": {},
   "source": [
    "\n",
    "------\n",
    "\n",
    "### ---- EXPLORACI√ìN 20 RATAS SANAS : FILTRADO, PROMEDIO Y SELECCI√ìN ---- Caso th = 0.2\n",
    "#### DATOS ALEJANDRO RATAS (1-20) 13.10.25\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dddb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.stats import entropy, wasserstein_distance\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "# HELPER FUNCTIONS\n",
    "\n",
    "# --- Construcci√≥n robusta del name_map con IDs reales ---\n",
    "def make_name_map_from_ids(roi_names, left_ids, right_ids):\n",
    "    \"\"\"\n",
    "    roi_names: lista de 78 nombres base en el orden correcto del atlas.\n",
    "    left_ids, right_ids: listas con los IDs REALES que corresponden a esos 78 nombres.\n",
    "                         Deben tener len()==len(roi_names).\n",
    "    Devuelve dict {roi_id: \"L-<name>\" / \"R-<name>\"}.\n",
    "    \"\"\"\n",
    "    if len(left_ids) != len(roi_names) or len(right_ids) != len(roi_names):\n",
    "        raise ValueError(\"left_ids y right_ids deben tener la misma longitud que roi_names (79).\")\n",
    "\n",
    "    name_map = {}\n",
    "    for k, rid in enumerate(left_ids):\n",
    "        name_map[int(rid)] = f\"L-{roi_names[k]}\"\n",
    "    for k, rid in enumerate(right_ids):\n",
    "        name_map[int(rid)] = f\"R-{roi_names[k]}\"\n",
    "    return name_map\n",
    "\n",
    "# --- Etiquetador que NO asume contig√ºidad ---\n",
    "def roi_label(idx, name_map):\n",
    "    \"\"\"\n",
    "    idx: ROI id real (disperso). name_map: dict {id: 'L-Name'/'R-Name'}.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return name_map[int(idx)]\n",
    "    except KeyError:\n",
    "        return f\"ID{int(idx)}\"  # fallback visible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd72865",
   "metadata": {},
   "source": [
    "## 2. Diferencias clave con el notebook de 1 rata\n",
    "\n",
    "| Aspecto | Notebook actual | Nuevo enfoque multi-rata |\n",
    "|---------|-----------------|--------------------------|\n",
    "| Input | `pickle` con delays crudos | `.dat` con fits (medias/stds) |\n",
    "| Estructura | `dict[(i,j)] ‚Üí array(N√ó6)` | Probablemente matriz o lista de par√°metros |\n",
    "| n_fibers | Por streamline | Agregado en el fit |\n",
    "| CV/dispersi√≥n | Calculado desde delays | Ya resumido o recalcular desde par√°metros |\n",
    "| Multi-sujeto | No aplica | Agregar/promediar entre 18 ratas |\n",
    "\n",
    "## 3. Flujo propuesto (adaptado)\n",
    "\n",
    "### Fase A: Carga y consolidaci√≥n\n",
    "1. **Leer todos los `.dat`** ‚Üí tabla unificada por rata\n",
    "2. **Estructura target**: `DataFrame` con columnas:\n",
    "   - `rat_id`, `roi_i`, `roi_j`, `n_fibers`, `tau_mean_ms`, `tau_std_ms`, ...\n",
    "3. **Filtrar conexiones**: `n_fibers ‚â• umbral` (50-100)\n",
    "\n",
    "### Fase B: An√°lisis por rata y agregado\n",
    "4. **M√©tricas por rata**:\n",
    "   - Distribuciones de œÑ por conexi√≥n\n",
    "   - Relaci√≥n œÑ~D (si D est√° en los fits)\n",
    "5. **Agregaci√≥n entre ratas**:\n",
    "   - Media/mediana de œÑ por conexi√≥n (i,j) across ratas\n",
    "   - Variabilidad inter-sujeto (CV entre ratas)\n",
    "6. **Selecci√≥n robusta**:\n",
    "   - Conexiones presentes en ‚â• N/2 ratas (e.g., ‚â•10/18)\n",
    "   - Bajo CV inter-rata\n",
    "   - Buen n_fibers promedio\n",
    "\n",
    "\n",
    "\n",
    "### Fase C: Categorizaci√≥n y clustering\n",
    "7. Aplicar misma l√≥gica del notebook:\n",
    "   - Intra/inter hemisf√©rico\n",
    "   - Hipocampo-PFC, t√°lamo-cortical\n",
    "   - Clustering por forma de distribuci√≥n (si hay par√°metros de fit suficientes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b79508",
   "metadata": {},
   "source": [
    "### 1. Carga consolidada - Nombres + Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3ae780",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../..\")\n",
    "\n",
    "path = './data/raw/rat_delays_fibers_0.2/th-0.2/'\n",
    "\n",
    "names = [f for f in os.listdir(path) if 'name' in f]\n",
    "\n",
    "# Abrir archivo .txt con nombres de ROIs y .dat con datos en formato diccionario\n",
    "with open(path+names[0], 'r') as f:\n",
    "    roi_names = [line.strip() for line in f.readlines()][1:]\n",
    "    \n",
    "print(roi_names), len(roi_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd42170d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_rats(data_dir, threshold='0.0'):\n",
    "    \"\"\"Carga 18 ratas ‚Üí dict {rat_id: data_dict}\"\"\"\n",
    "    rats = {}\n",
    "    path = Path(data_dir) / f'rat_delays_fibers_{threshold}' / f'th-{threshold}'\n",
    "    \n",
    "    for f in path.glob(f'th-{threshold}_R*_b20_r_Fit_Histogram_Tau_all_fibers.dat'):\n",
    "        rat_id = f.stem.split('_')[1]  # 'R01', 'R02', etc.\n",
    "        with open(f, 'rb') as fh:\n",
    "            rats[rat_id] = pickle.load(fh)\n",
    "    \n",
    "    return rats\n",
    "\n",
    "# Uso\n",
    "data_dir = './data/raw/'\n",
    "all_rats = load_all_rats(data_dir, threshold='0.2')\n",
    "print(f\"Ratas cargadas: {sorted(all_rats.keys())}\")  # R01-R19 (sin R11)\n",
    "print(f\"Ejemplo estructura R01: {len(all_rats['R01'])} conexiones\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0241852e",
   "metadata": {},
   "source": [
    "### Celda 4: Name map y exploraci√≥n inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1b95dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_ids = range(0, 78)   # IDs del 1 al 78 para hemisferio izquierdo\n",
    "right_ids = range(78, 156)\n",
    "\n",
    "name_map = make_name_map_from_ids(roi_names, left_ids, right_ids)\n",
    "\n",
    "# Exploraci√≥n: conexiones comunes entre ratas\n",
    "all_pairs = Counter()\n",
    "for rat_data in all_rats.values():\n",
    "    all_pairs.update(rat_data.keys())\n",
    "\n",
    "print(f\"Total conexiones √∫nicas: {len(all_pairs)}\")\n",
    "print(f\"Conexiones en ‚â•9 ratas: {sum(1 for c in all_pairs.values() if c >= 9)}\")\n",
    "print(f\"\\nTop 10 conexiones m√°s frecuentes:\")\n",
    "for (i,j), count in all_pairs.most_common(10):\n",
    "    print(f\"  {roi_label(i, name_map)} ‚Üí {roi_label(j, name_map)}: {count} ratas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73e64fd",
   "metadata": {},
   "source": [
    "Perfecto, los primeros pasos son correctos:\n",
    "\n",
    "## Celdas 3-4: Carga y Exploraci√≥n ‚úÖ\n",
    "\n",
    "**Carga th=0.0**:\n",
    "- 18 ratas cargadas (R01-R19, sin R11)\n",
    "- R01 ejemplo: 3937 conexiones raw\n",
    "\n",
    "**Exploraci√≥n inicial**:\n",
    "- **7259 conexiones √∫nicas** entre todas las ratas\n",
    "- **3409 conexiones robustas** (‚â•9 ratas) - ~47% del total\n",
    "- Top 10: **todas con 18/18 ratas** (m√°xima consistencia)\n",
    "\n",
    "**Patr√≥n dominante**: L-Olfactory bulb como hub principal (aparece en 8/10 top)\n",
    "\n",
    "**Observaci√≥n importante**: Las conexiones mostradas son **intra-hemisf√©ricas izquierdas** (L‚ÜíL), lo cual es esperado ya que el bulbo olfatorio tiene conectividad extensa ipsilateral.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5599dc9a",
   "metadata": {},
   "source": [
    "### Celda 5: Limpieza por rata\n",
    "    - Min fibers: 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13361e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columnas (D y V no se usan en la limpieza; se dejan por compatibilidad)\n",
    "COL_TAU = 0\n",
    "COL_D   = 1\n",
    "COL_V   = 2\n",
    "\n",
    "def clean_data(\n",
    "    data: dict,\n",
    "    *,\n",
    "    min_n_fibers: int = 50,\n",
    "    enforce_positive: bool = True,\n",
    "    tau_quantiles: tuple[float, float] | None = (0.005, 0.995),\n",
    ") -> tuple[dict, pd.DataFrame, dict]:\n",
    "    \"\"\"\n",
    "    Limpia mediciones por par (i,j) sin chequeo œÑ‚âàD/V.\n",
    "    - Filtros: finitos, (opcional) œÑ,D,V > 0, cuantiles de œÑ por par.\n",
    "    - Umbral min_n_fibers antes y despu√©s de limpiar.\n",
    "    Devuelve:\n",
    "      \n",
    "      cleaned_data: dict[(i,j)] -> ndarray float32 (m, >=3)\n",
    "      pair_summary: DF con n_raw, n_clean y medianas (œÑ,D,V)\n",
    "      stats: contadores de pares y filas\n",
    "    \"\"\"\n",
    "    cleaned_data = {}\n",
    "\n",
    "    pair_stats = {\n",
    "        \"pairs_original\": len(data),\n",
    "        \"pairs_empty_raw\": 0,\n",
    "        \"pairs_raw_lt_min\": 0,\n",
    "        \"pairs_all_invalid\": 0,\n",
    "        \"pairs_after_lt_min\": 0,\n",
    "        \"pairs_kept\": 0,\n",
    "    }\n",
    "    row_stats = {\n",
    "        \"rows_total\": 0,\n",
    "        \"rows_kept\": 0,\n",
    "        \"rows_drop_nan_inf\": 0,\n",
    "        \"rows_drop_nonpositive\": 0,\n",
    "        \"rows_drop_outlier_tau\": 0,\n",
    "    }\n",
    "\n",
    "    rows_summary = []\n",
    "\n",
    "    for (i, j), measurements in data.items():\n",
    "        if measurements is None or len(measurements) == 0:\n",
    "            pair_stats[\"pairs_empty_raw\"] += 1\n",
    "            continue\n",
    "\n",
    "        arr = np.asarray(measurements)\n",
    "        # exigimos al menos œÑ,D,V (>=3 columnas)\n",
    "        if arr.ndim != 2 or arr.shape[1] < 3:\n",
    "            pair_stats[\"pairs_all_invalid\"] += 1\n",
    "            continue\n",
    "\n",
    "        n_raw = arr.shape[0]\n",
    "        row_stats[\"rows_total\"] += n_raw\n",
    "        if n_raw < min_n_fibers:\n",
    "            pair_stats[\"pairs_raw_lt_min\"] += 1\n",
    "            continue\n",
    "\n",
    "        # Finite en œÑ,D,V\n",
    "        finite = np.isfinite(arr[:, [COL_TAU, COL_D, COL_V]]).all(axis=1)\n",
    "        row_stats[\"rows_drop_nan_inf\"] += int((~finite).sum())\n",
    "        arr = arr[finite]\n",
    "        if arr.size == 0:\n",
    "            pair_stats[\"pairs_all_invalid\"] += 1\n",
    "            continue\n",
    "\n",
    "        # Positivos (opcional)\n",
    "        if enforce_positive:\n",
    "            pos = (arr[:, COL_TAU] > 0) & (arr[:, COL_D] > 0) & (arr[:, COL_V] > 0)\n",
    "            row_stats[\"rows_drop_nonpositive\"] += int((~pos).sum())\n",
    "            arr = arr[pos]\n",
    "            if arr.size == 0:\n",
    "                pair_stats[\"pairs_all_invalid\"] += 1\n",
    "                continue\n",
    "\n",
    "        # Outliers de œÑ por cuantiles (por par)\n",
    "        if tau_quantiles is not None and arr.shape[0] >= 5:\n",
    "            qlo, qhi = tau_quantiles\n",
    "            tau_vals = arr[:, COL_TAU]\n",
    "            lo = np.nanquantile(tau_vals, qlo)\n",
    "            hi = np.nanquantile(tau_vals, qhi)\n",
    "            in_rng = (tau_vals >= lo) & (tau_vals <= hi)\n",
    "            row_stats[\"rows_drop_outlier_tau\"] += int((~in_rng).sum())\n",
    "            arr = arr[in_rng]\n",
    "            if arr.size == 0:\n",
    "                pair_stats[\"pairs_all_invalid\"] += 1\n",
    "                continue\n",
    "\n",
    "        n_clean = arr.shape[0]\n",
    "        if n_clean < min_n_fibers:\n",
    "            pair_stats[\"pairs_after_lt_min\"] += 1\n",
    "            continue\n",
    "\n",
    "        cleaned = arr.astype(np.float32, copy=False)\n",
    "        cleaned_data[(int(i), int(j))] = cleaned\n",
    "        pair_stats[\"pairs_kept\"] += 1\n",
    "        row_stats[\"rows_kept\"] += n_clean\n",
    "\n",
    "        # Resumen por par\n",
    "        med_tau = float(np.median(cleaned[:, COL_TAU]))\n",
    "        med_D   = float(np.median(cleaned[:, COL_D]))\n",
    "        med_V   = float(np.median(cleaned[:, COL_V]))\n",
    "\n",
    "        rows_summary.append({\n",
    "            \"roi_i\": int(i), \"roi_j\": int(j),\n",
    "            \"roi_name1\": roi_label(i, name_map), \"roi_name2\": roi_label(j, name_map),\n",
    "            \"n_raw\": int(n_raw), \"n_clean\": int(n_clean),\n",
    "            \"tau_med_s\": med_tau, \"tau_med_ms\": med_tau*1e3,\n",
    "            \"D_med_m\": med_D, \"D_med_mm\": med_D*1e3,\n",
    "            \"V_med_mps\": med_V,\n",
    "        })\n",
    "\n",
    "    pair_summary = pd.DataFrame(rows_summary).sort_values([\"roi_i\", \"roi_j\"]).reset_index(drop=True)\n",
    "    stats = {\"pairs\": pair_stats, \"rows\": row_stats}\n",
    "    return cleaned_data, pair_summary, stats\n",
    "\n",
    "\n",
    "cleaned_rats = {}\n",
    "summaries = {}\n",
    "stats = {}\n",
    "for rat_id, data in all_rats.items():\n",
    "    cleaned_rats[rat_id], summaries[rat_id], stats[rat_id] = clean_data(\n",
    "        data, min_n_fibers=50, enforce_positive=True, tau_quantiles=(0.0, 1.0)\n",
    "    )\n",
    "cleaned_rats.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385ab5f5",
   "metadata": {},
   "source": [
    "### - Mostramos los resultados para una rata: 02\n",
    "\n",
    "    - Claves de pares de ROIs (i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83280356",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_rats['R02'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfed2b76",
   "metadata": {},
   "source": [
    "### - Resumen de estad√≠sticas descriptivas: Pares, nombres, n_fibras_raw vs n_fibras_clean, medianas de: tau, distancia, velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0167fb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries['R02']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62365ce3",
   "metadata": {},
   "source": [
    "- ### Original vs kept pairs, n_rows...\n",
    "  \n",
    "    - Configuraci√≥n usada:\n",
    "\n",
    "        - min_n_fibers=25 (m√°s permisivo que el t√≠pico 50)\n",
    "        - tau_quantiles=(0.0, 1.0) ‚Üí sin filtrado de outliers œÑ\n",
    "        - Mantiene positivos y finitos\n",
    "\n",
    "    - Resultado R02 (ejemplo):\n",
    "\n",
    "        - 3283 ‚Üí 1392 pares (42% retenido)\n",
    "        - 468K ‚Üí 459K filas (98% streamlines OK)\n",
    "        - P√©rdidas principales: pares con n<25 (1282) y vac√≠os (605)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9e1ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats['R02']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef24801",
   "metadata": {},
   "source": [
    "- ### Agregaci√≥n inter-rata con m√©tricas clave\n",
    "  - Ordenado por tau_range_mean (prioriza diversidad temporal) + n_rats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7061dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_multi_rat(cleaned_rats, min_rats=10):\n",
    "    \"\"\"\n",
    "    Consolida conexiones presentes en ‚â• min_rats.\n",
    "    Devuelve DataFrame con estad√≠sticas inter-rata.\n",
    "    \"\"\"\n",
    "    \n",
    "    conn_data = defaultdict(lambda: {\n",
    "        'rats': [], 'tau_med_ms': [], 'tau_range_ms': [], \n",
    "        'n_fibers': [], 'D_med_mm': []\n",
    "    })\n",
    "    \n",
    "    for rat_id, data in cleaned_rats.items():\n",
    "        for (i,j), arr in data.items():\n",
    "            tau_ms = arr[:, COL_TAU] * 1e3\n",
    "            D_mm = arr[:, COL_D] * 1e3\n",
    "            \n",
    "            conn_data[(i,j)]['rats'].append(rat_id)\n",
    "            conn_data[(i,j)]['tau_med_ms'].append(np.median(tau_ms))\n",
    "            conn_data[(i,j)]['tau_range_ms'].append(np.ptp(tau_ms))  # max-min\n",
    "            conn_data[(i,j)]['n_fibers'].append(len(tau_ms))\n",
    "            conn_data[(i,j)]['D_med_mm'].append(np.median(D_mm))\n",
    "    \n",
    "    rows = []\n",
    "    for (i,j), stats in conn_data.items():\n",
    "        n_rats = len(stats['rats'])\n",
    "        if n_rats < min_rats:\n",
    "            continue\n",
    "        \n",
    "        tau_vals = np.array(stats['tau_med_ms'])\n",
    "        rows.append({\n",
    "            'roi_i': int(i), 'roi_j': int(j),\n",
    "            'pair_label': f\"{roi_label(i, name_map)} ‚Üí {roi_label(j, name_map)}\",\n",
    "            'n_rats': n_rats,\n",
    "            'tau_mean_ms': tau_vals.mean(),\n",
    "            'tau_std_inter': tau_vals.std(),           # variabilidad entre ratas\n",
    "            'cv_inter': tau_vals.std() / tau_vals.mean(),\n",
    "            'tau_range_mean': np.mean(stats['tau_range_ms']),  # rango promedio\n",
    "            'n_fibers_mean': np.mean(stats['n_fibers']),\n",
    "            'D_mean_mm': np.mean(stats['D_med_mm']),\n",
    "            'hemi': 'intra' if (i < 78 and j < 78) or (i >= 78 and j >= 78) else 'inter',\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    return df.sort_values(['tau_range_mean', 'n_rats'], ascending=[False, False])\n",
    "\n",
    "df_multi = aggregate_multi_rat(cleaned_rats, min_rats=9)\n",
    "print(f\"Conexiones con ‚â•10 ratas: {len(df_multi)}\")\n",
    "df_multi.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741183f3",
   "metadata": {},
   "source": [
    "## Resultados Agregaci√≥n ‚úÖ\n",
    "\n",
    "**1635 conexiones robustas** (‚â•9 ratas) - excelente cobertura.\n",
    "\n",
    "### An√°lisis Top 20:\n",
    "\n",
    "**Patrones anat√≥micos**:\n",
    "- 100% **intra-hemisf√©ricas** (esperado para delays largos)\n",
    "- 70% hemisferio **derecho** (R‚ÜíR)\n",
    "- **Subthalamic nucleus** = hub (6/20 conexiones)\n",
    "\n",
    "**Calidad temporal**:\n",
    "- œÑ_range: **4.3-5.0 ms** - diversidad √≥ptima\n",
    "- CV inter-rata: **0.11-0.52** (mayor√≠a <0.35) - buena robustez\n",
    "- n_fibers: **122-5436** - suficiente\n",
    "\n",
    "**Conexiones anat√≥micas de inter√©s**:\n",
    "- #8: **Parietal ‚Üí Subiculum** (3233 fibers, CV=0.28) ‚Üê Corteza-l√≠mbico\n",
    "- #10: **Accumbens ‚Üí Motor** (707 fibers, CV=0.20) ‚Üê L√≠mbico-motor\n",
    "- #4: **Endopiriform ‚Üí Prelimbic** (1060 fibers, CV=0.19) ‚Üê L√≠mbico-PFC\n",
    "\n",
    "**Observaci√≥n**: Falta diversidad inter-hemisf√©rica. Deber√° buscarse en candidatos con œÑ_range >3ms pero <4.3ms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb90d8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_outlier_removal(cleaned_rats, min_rats=8):\n",
    "    \"\"\"\n",
    "    Elimina fibras/conexiones outliers usando consenso entre ratas.\n",
    "    \n",
    "    Para cada conexi√≥n (i,j):\n",
    "    1. Calcula percentiles P5-P95 de tau_median por rata\n",
    "    2. Elimina ratas con tau fuera del rango inter-rata\n",
    "    3. Dentro de cada rata, elimina fibras outliers (MAD > 3)\n",
    "    4. Descarta conexi√≥n si CV_inter > 0.5 o quedan <min_rats\n",
    "    \"\"\"\n",
    "    \n",
    "    refined_rats = {}\n",
    "    removal_log = []\n",
    "    \n",
    "    # Paso 1: Identificar conexiones compartidas\n",
    "    conn_inventory = defaultdict(list)\n",
    "    for rat_id, data in cleaned_rats.items():\n",
    "        for (i,j) in data.keys():\n",
    "            conn_inventory[(i,j)].append(rat_id)\n",
    "    \n",
    "    # Filtrar solo conexiones con suficientes ratas\n",
    "    valid_conns = {k: v for k, v in conn_inventory.items() if len(v) >= min_rats}\n",
    "    print(f\"Conexiones con ‚â•{min_rats} ratas: {len(valid_conns)}\")\n",
    "    \n",
    "    # Paso 2: Limpieza por conexi√≥n\n",
    "    for (i,j), rat_list in valid_conns.items():\n",
    "        # Recopilar medianas por rata\n",
    "        tau_medians = []\n",
    "        for rat_id in rat_list:\n",
    "            tau_ms = cleaned_rats[rat_id][(i,j)][:, COL_TAU] * 1e3\n",
    "            tau_medians.append(np.median(tau_ms))\n",
    "        \n",
    "        tau_medians = np.array(tau_medians)\n",
    "        \n",
    "        # Detectar ratas outlier (fuera de P5-P95 inter-rata)\n",
    "        p5, p95 = np.percentile(tau_medians, [5, 95])\n",
    "        valid_rats = [rat_list[k] for k in range(len(rat_list)) \n",
    "                      if p5 <= tau_medians[k] <= p95]\n",
    "        \n",
    "        if len(valid_rats) < min_rats:\n",
    "            removal_log.append({\n",
    "                'pair': (i,j), 'reason': 'insufficient_rats_after_outlier',\n",
    "                'n_rats_before': len(rat_list), 'n_rats_after': len(valid_rats)\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # CV inter-rata (solo con ratas v√°lidas)\n",
    "        valid_medians = [tau_medians[rat_list.index(r)] for r in valid_rats]\n",
    "        cv_inter = np.std(valid_medians) / np.mean(valid_medians)\n",
    "        \n",
    "        if cv_inter > 0.5:\n",
    "            removal_log.append({\n",
    "                'pair': (i,j), 'reason': 'high_cv_inter',\n",
    "                'cv': cv_inter, 'n_rats': len(valid_rats)\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # Paso 3: Guardar conexiones v√°lidas (sin MAD intra-rata)\n",
    "        # Solo aplicamos filtros inter-rata, preservando colas largas\n",
    "        for rat_id in valid_rats:\n",
    "            if rat_id not in refined_rats:\n",
    "                refined_rats[rat_id] = {}\n",
    "            \n",
    "            # Mantener datos originales (ya limpiados en clean_data)\n",
    "            refined_rats[rat_id][(i,j)] = cleaned_rats[rat_id][(i,j)]\n",
    "    \n",
    "    print(f\"\\nRefinamiento completado:\")\n",
    "    print(f\"  Ratas procesadas: {len(refined_rats)}\")\n",
    "    print(f\"  Conexiones eliminadas: {len(removal_log)}\")\n",
    "    \n",
    "    # Resumen de razones de eliminaci√≥n\n",
    "    reasons = pd.DataFrame(removal_log)\n",
    "    if len(reasons) > 0:\n",
    "        print(\"\\nMotivos de eliminaci√≥n:\")\n",
    "        print(reasons['reason'].value_counts())\n",
    "    \n",
    "    return refined_rats, removal_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac5e2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar limpieza avanzada\n",
    "print(\"=\"*70)\n",
    "print(\"LIMPIEZA AVANZADA POR CONSENSO INTER-RATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "refined_rats, log = advanced_outlier_removal(cleaned_rats, min_rats=9)\n",
    "\n",
    "# Reagregar con datos refinados\n",
    "df_refined = aggregate_multi_rat(refined_rats, min_rats=9)\n",
    "\n",
    "print(f\"\\n{'COMPARACI√ìN':=^70}\")\n",
    "print(f\"Antes:    {len(df_multi)} conexiones robustas\")\n",
    "print(f\"Despu√©s:  {len(df_refined)} conexiones refinadas\")\n",
    "print(f\"P√©rdida:  {len(df_multi) - len(df_refined)} ({100*(len(df_multi)-len(df_refined))/len(df_multi):.1f}%)\")\n",
    "\n",
    "# Top 20 despu√©s de refinamiento\n",
    "print(f\"\\n{'TOP 20 REFINADAS':=^70}\")\n",
    "print(df_refined.head(20)[['pair_label', 'n_rats', 'tau_range_mean', \n",
    "                            'cv_inter', 'n_fibers_mean']].to_string(index=False))\n",
    "\n",
    "# # Guardar\n",
    "# df_refined.to_csv('../../results/data_analysis/refined_th_0.0.csv', index=False)\n",
    "# print(\"\\n‚úì Guardado: refined_th_0.0.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc64f24d",
   "metadata": {},
   "source": [
    "## Refinamiento ‚úÖ\n",
    "\n",
    "**Eliminadas**: 354 conexiones (13.6%)\n",
    "- 230 por ratas outliers\n",
    "- 124 por CV>0.5\n",
    "\n",
    "**Top 20 refinadas**: œÑ_range 4.2-5.0 ms, CV <0.35 (excepto #5 y #8)\n",
    "\n",
    "**Mejoras**:\n",
    "- ‚Üì CV promedio: 0.28‚Üí0.21\n",
    "- ‚Üë robustez: mayor√≠a 16/16 ratas\n",
    "- Mantiene diversidad anat√≥mica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c10ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar ambos\n",
    "df_02 = pd.read_csv('./results/data_analysis/results_th_0.2.csv')\n",
    "df_00 = df_refined.copy()\n",
    "\n",
    "# Pares comunes\n",
    "pairs_02 = set(zip(df_02['roi_i'], df_02['roi_j']))\n",
    "pairs_00 = set(zip(df_00['roi_i'], df_00['roi_j']))\n",
    "\n",
    "common = pairs_02 & pairs_00\n",
    "print(f\"Comunes: {len(common)}\")\n",
    "print(f\"Solo en 0.0: {len(pairs_00 - pairs_02)}\")\n",
    "print(f\"Solo en 0.2: {len(pairs_02 - pairs_00)}\")\n",
    "\n",
    "# Top 20 coincidentes\n",
    "top20_00 = set(zip(df_00.head(20)['roi_i'], df_00.head(20)['roi_j']))\n",
    "top20_02 = set(zip(df_02.nlargest(20, 'tau_range_mean')['roi_i'], \n",
    "                   df_02.nlargest(20, 'tau_range_mean')['roi_j']))\n",
    "print(f\"\\nTop 20 coinciden: {len(top20_00 & top20_02)}/20\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d089ae16",
   "metadata": {},
   "source": [
    "### Top 20 con mayor rango temporal (2.4-2.8 ms):\n",
    "- Patrones clave:\n",
    "\n",
    "  - Todos intra-hemisf√©ricos subcorticales/l√≠mbicos\n",
    "  - 18/18 ratas en todos (m√°xima robustez)\n",
    "  - CV_inter: 0.12-0.55 (algunos muy estables, otros m√°s variables entre sujetos)\n",
    "  - Protagonistas: Hipot√°lamo, Subiculum, PAG, Zona incerta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484cefef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criterios configurables\n",
    "MIN_RATS_FILTER = 9\n",
    "MAX_CV_INTER = 0.15\n",
    "MIN_FIBERS_FILTER = 100\n",
    "MIN_TAU_RANGE = 2.5\n",
    "\n",
    "# Filtrado\n",
    "df_stable_diverse = df_refined[\n",
    "    (df_refined['n_rats'] >= MIN_RATS_FILTER) &\n",
    "    (df_refined['cv_inter'] < MAX_CV_INTER) &\n",
    "    (df_refined['n_fibers_mean'] >= MIN_FIBERS_FILTER) &\n",
    "    (df_refined['tau_range_mean'] > MIN_TAU_RANGE)\n",
    "].sort_values('tau_range_mean', ascending=False)\n",
    "\n",
    "print(f\"Candidatos: {len(df_stable_diverse )} (œÑ>{MIN_TAU_RANGE}ms, n>{MIN_FIBERS_FILTER}, CV<{MAX_CV_INTER})\")\n",
    "\n",
    "# Exploraciones\n",
    "print(\"\\nTop 20 por n_fibers:\")\n",
    "display(df_refined.nlargest(20, 'n_fibers_mean')[['pair_label', 'n_fibers_mean', 'tau_range_mean']])\n",
    "\n",
    "print(\"\\nTop 20 por tau_range:\")\n",
    "display(df_refined.nlargest(20, 'tau_range_mean')[['pair_label', 'tau_range_mean', 'n_rats', 'cv_inter']])\n",
    "\n",
    "df_stable_diverse.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d513bf2",
   "metadata": {},
   "source": [
    "## Filtrado Final ‚úÖ\n",
    "\n",
    "**319 candidatos** con œÑ>2.5ms, n‚â•100, CV<0.5\n",
    "\n",
    "### An√°lisis por Prioridad:\n",
    "\n",
    "**Por œÑ_range** (top 15):\n",
    "- 4.2-5.0 ms, CV<0.35\n",
    "- **Subthalamic nucleus** sigue dominando\n",
    "- #3: **Endopiriform‚ÜíPrelimbic** (1058f, CV=0.16) ‚Üê PFC\n",
    "- #6: **Parietal‚ÜíSubiculum** (3207f, CV=0.21) ‚Üê Corteza-l√≠mbico\n",
    "- #10: **Accumbens‚ÜíMotor** (701f, CV=0.16) ‚Üê L√≠mbico-motor\n",
    "\n",
    "**Por n_fibers** (top 20):\n",
    "- Miles de fibras pero **œÑ_range m√°s bajo** (1.9-3.8ms)\n",
    "- Dominan: **Subiculum, Ventral striatal, PAG**\n",
    "- Balance calidad/cantidad\n",
    "\n",
    "### Problema: **0% inter-hemisf√©ricas**\n",
    "\n",
    "Para diversidad anat√≥mica, necesitas:\n",
    "1. Bajar umbral a œÑ>2.0ms o revisar hemi=='inter'\n",
    "2. Clustering sobre 319 para identificar morfolog√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a778c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tau_distributions_multirat(rats_data, pair, name_map, bins=50):\n",
    "    \"\"\"Histogramas œÑ por rata (grid 3√ó6)\"\"\"\n",
    "    fig, axes = plt.subplots(3, 6, figsize=(18, 9))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for idx, (rat_id, data) in enumerate(sorted(rats_data.items())):\n",
    "        if pair not in data:\n",
    "            axes[idx].text(0.5, 0.5, 'N/A', ha='center', va='center')\n",
    "            axes[idx].set_title(rat_id)\n",
    "            axes[idx].axis('off')\n",
    "            continue\n",
    "        \n",
    "        tau_ms = data[pair][:, COL_TAU] * 1e3\n",
    "        axes[idx].hist(tau_ms, bins=bins, alpha=0.75, edgecolor='k', lw=0.5)\n",
    "        axes[idx].axvline(np.median(tau_ms), color='r', ls='--', lw=1.5)\n",
    "        axes[idx].set_title(f\"{rat_id} (n={len(tau_ms)})\", fontsize=9)\n",
    "        axes[idx].set_xlabel('œÑ (ms)', fontsize=8)\n",
    "    \n",
    "    i, j = pair\n",
    "    fig.suptitle(f\"{roi_label(i, name_map)} ‚Üí {roi_label(j, name_map)}\", fontsize=13, y=0.995)\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Visualizar top 3\n",
    "for idx in [0, 1, 2,3,4,5]:\n",
    "    row = df_stable_diverse.iloc[idx]\n",
    "    pair = (row['roi_i'], row['roi_j'])\n",
    "    plot_tau_distributions_multirat(refined_rats, pair, name_map, bins=50)\n",
    "    print(pair)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9af023f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ==============================================================\n",
    "# # üß© EXPLORACI√ìN MANUAL DE DISTRIBUCIONES ‚Äî SELECCI√ìN VISUAL\n",
    "# # ==============================================================\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import pandas as pd\n",
    "\n",
    "# # --------------------------------------------------------------\n",
    "# # 1Ô∏è‚É£ Selecci√≥n de conexiones candidatas\n",
    "# # --------------------------------------------------------------\n",
    "\n",
    "# # Filtramos las conexiones estables y ricas en fibras\n",
    "# df_candidates = df_multi[\n",
    "#     (df_multi['n_rats'] >= 4) &\n",
    "#     (df_multi['cv_inter'] < 0.1) &\n",
    "#     (df_multi['n_fibers_mean'] >= 50) &\n",
    "#     (df_multi['tau_range_mean'] > 2.0)\n",
    "# ].copy()\n",
    "\n",
    "# # Ordenar por rango temporal y robustez\n",
    "# df_candidates = df_candidates.sort_values(\n",
    "#     ['tau_range_mean', 'n_fibers_mean'],\n",
    "#     ascending=[False, False]\n",
    "# ).reset_index(drop=True)\n",
    "\n",
    "# print(f\"‚úÖ {len(df_candidates)} conexiones cumplen criterios\")\n",
    "# display(df_candidates.head(15)[[\n",
    "#     'pair_label', 'tau_range_mean', 'cv_inter', 'n_fibers_mean',\n",
    "#     'n_rats', 'hemi'\n",
    "# ]])\n",
    "\n",
    "# # --------------------------------------------------------------\n",
    "# # 2Ô∏è‚É£ Visualizaci√≥n del top N\n",
    "# # --------------------------------------------------------------\n",
    "\n",
    "# def plot_multi_histogram(pair, cleaned_rats, name_map, bins=50):\n",
    "#     \"\"\"Visualiza distribuciones œÑ (ms) por rata en un grid.\"\"\"\n",
    "#     i, j = pair\n",
    "#     fig, axes = plt.subplots(3, 6, figsize=(18, 9))\n",
    "#     axes = axes.ravel()\n",
    "\n",
    "#     for idx, (rat_id, data) in enumerate(sorted(cleaned_rats.items())):\n",
    "#         ax = axes[idx]\n",
    "#         if pair not in data:\n",
    "#             ax.text(0.5, 0.5, 'N/A', ha='center', va='center')\n",
    "#             ax.set_title(rat_id)\n",
    "#             ax.axis('off')\n",
    "#             continue\n",
    "\n",
    "#         tau_ms = data[pair][:, COL_TAU] * 1e3\n",
    "#         ax.hist(tau_ms, bins=bins, alpha=0.75, edgecolor='k', linewidth=0.5)\n",
    "#         ax.axvline(np.median(tau_ms), color='r', ls='--', lw=1)\n",
    "#         ax.set_title(f\"{rat_id} (n={len(tau_ms)})\", fontsize=8)\n",
    "#         ax.set_xlabel('œÑ (ms)', fontsize=7)\n",
    "#         ax.set_ylabel('freq', fontsize=7)\n",
    "\n",
    "#     fig.suptitle(f\"{roi_label(i, name_map)} ‚Üí {roi_label(j, name_map)}\", fontsize=13)\n",
    "#     fig.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# # --------------------------------------------------------------\n",
    "# # 3Ô∏è‚É£ Iterar visualmente sobre el top\n",
    "# # --------------------------------------------------------------\n",
    "\n",
    "# TOP_N = 10  # n√∫mero de conexiones que quieres explorar\n",
    "\n",
    "# for idx in range(TOP_N):\n",
    "#     row = df_candidates.iloc[idx]\n",
    "#     pair = (int(row.roi_i), int(row.roi_j))\n",
    "\n",
    "#     print(\"=\"*80)\n",
    "#     print(f\"#{idx+1}  {row.pair_label}\")\n",
    "#     print(f\"  œÑ_range_mean = {row.tau_range_mean:.2f} ms\")\n",
    "#     print(f\"  n_fibers_mean = {row.n_fibers_mean:.0f}\")\n",
    "#     print(f\"  n_rats = {row.n_rats}, CV_inter = {row.cv_inter:.2f}, hemi = {row.hemi}\")\n",
    "#     plot_multi_histogram(pair, cleaned_rats, name_map, bins=40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7595c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stable_diverse['pair_label'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a76bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_rats['R01'][(29, 70)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b497d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_names[28], roi_names[69]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8177d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair = (29, 70)\n",
    "plot_tau_distributions_multirat(cleaned_rats, pair, name_map, bins=40)\n",
    "print(pair)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cf0cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair = (28, 69)\n",
    "plot_avg_distribution(pair, refined_rats, name_map, 50, False)\n",
    "print(pair)\n",
    "print(label)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff206a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# üß© DISTRIBUCIONES PROMEDIADAS MULTI-RATA + DESCARTE OUTLIERS (¬±3œÉ)\n",
    "# ==============================================================\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "def plot_avg_distribution(pair, cleaned_rats, name_map, bins=75, save=False, sigma_thresh=1.0):\n",
    "    \"\"\"\n",
    "    Calcula y muestra el histograma promedio multi-rata con exclusi√≥n de outliers (>3œÉ).\n",
    "    Devuelve (centers, mean_hist, kept_rats)\n",
    "    \"\"\"\n",
    "    i, j = pair\n",
    "    all_hists, all_edges, valid_rats = [], [], []\n",
    "\n",
    "    # --- Calcular histogramas individuales normalizados ---\n",
    "    for rat_id, data in cleaned_rats.items():\n",
    "        if pair not in data:\n",
    "            continue\n",
    "        tau_ms = data[pair][:, COL_TAU] * 1e3\n",
    "        hist, edges = np.histogram(tau_ms, bins=bins, density=True)\n",
    "        all_hists.append(hist)\n",
    "        all_edges.append(edges)\n",
    "        valid_rats.append(rat_id)\n",
    "\n",
    "    if not all_hists:\n",
    "        print(f\"‚ö†Ô∏è Sin datos suficientes para {roi_label(i, name_map)} ‚Üí {roi_label(j, name_map)}\")\n",
    "        return None, None, []\n",
    "\n",
    "    # --- Verificar consistencia de bins ---\n",
    "    edges = all_edges[0]\n",
    "    all_hists = np.array([h for h in all_hists if len(h) == len(edges) - 1])\n",
    "\n",
    "    # --- Calcular media y desviaci√≥n inicial ---\n",
    "    mean_init = all_hists.mean(axis=0)\n",
    "    std_init = all_hists.std(axis=0)\n",
    "\n",
    "    # --- Evaluar distancia tipo z-score promedio por rata ---\n",
    "    z_scores = []\n",
    "    for h in all_hists:\n",
    "        z = np.abs(h - mean_init) / (std_init + 1e-8)\n",
    "        z_mean = np.nanmean(z)\n",
    "        z_scores.append(z_mean)\n",
    "    z_scores = np.array(z_scores)\n",
    "\n",
    "    # --- Filtrar ratas dentro de umbral (3œÉ) ---\n",
    "    keep_mask = z_scores < sigma_thresh\n",
    "    kept_hists = all_hists[keep_mask]\n",
    "    kept_rats = np.array(valid_rats)[keep_mask]\n",
    "\n",
    "    # --- Recalcular promedio y desviaci√≥n final ---\n",
    "    mean_hist = kept_hists.mean(axis=0)\n",
    "    std_hist = kept_hists.std(axis=0)\n",
    "    centers = (edges[:-1] + edges[1:]) / 2\n",
    "\n",
    "    # --- Plot ---\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(centers, mean_hist, color='royalblue', lw=2, label='Media (post-filtrado)')\n",
    "    plt.fill_between(centers, mean_hist - std_hist, mean_hist + std_hist,\n",
    "                     color='lightblue', alpha=0.4, label='¬±1œÉ inter-rata')\n",
    "    plt.xlabel('Delay œÑ (ms)')\n",
    "    plt.ylabel('Densidad normalizada')\n",
    "    plt.title(f\"{roi_label(i, name_map)} ‚Üí {roi_label(j, name_map)}\\n\"\n",
    "              f\"Ratas v√°lidas: {len(kept_rats)}/{len(valid_rats)} (outliers: {len(valid_rats)-len(kept_rats)})\")\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- Exportar opcionalmente los valores promedio ---\n",
    "    if save:\n",
    "        export_dir = Path('./data/exports/avg_distributions_filtered')\n",
    "        export_dir.mkdir(parents=True, exist_ok=True)\n",
    "        np.savez(\n",
    "            export_dir / f\"avgdist_{roi_label(i, name_map)}_to_{roi_label(j, name_map)}.npz\",\n",
    "            centers=centers,\n",
    "            mean_hist=mean_hist,\n",
    "            std_hist=std_hist,\n",
    "            kept_rats=kept_rats\n",
    "        )\n",
    "        print(f\"‚úÖ Exportado promedio multi-rata (filtrado 3œÉ): \"\n",
    "              f\"{roi_label(i, name_map)} ‚Üí {roi_label(j, name_map)}\")\n",
    "\n",
    "    return centers, mean_hist, kept_rats\n",
    "\n",
    "\n",
    "for idx in range(len(df_stable_diverse)):\n",
    "    \n",
    "    label = df_stable_diverse.iloc[idx]['pair_label']\n",
    "    \n",
    "    if \"Hippo\" in label:\n",
    "\n",
    "        row = df_stable_diverse.iloc[idx]\n",
    "        pair = (row['roi_i'], row['roi_j'])\n",
    "        plot_avg_distribution(pair, refined_rats, name_map, 50, False)\n",
    "        print(pair)\n",
    "        print(label)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a072b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis sistem√°tico de las 15 top conexiones\n",
    "results = []\n",
    "\n",
    "for idx in range(min(5, len(df_stable_diverse))):\n",
    "    row = df_stable_diverse.iloc[idx]\n",
    "    pair = (row['roi_i'], row['roi_j'])\n",
    "    \n",
    "    centers, mean_hist, kept_rats = plot_avg_distribution(\n",
    "        pair, refined_rats, name_map, bins=50, save=False, sigma_thresh=3.0\n",
    "    )\n",
    "    \n",
    "    if centers is not None and len(centers) > 0:\n",
    "        # Detectar modalidad\n",
    "        from scipy.signal import find_peaks\n",
    "        peaks, _ = find_peaks(mean_hist, height=mean_hist.max()*0.3, distance=5)\n",
    "        n_modes = len(peaks)\n",
    "        \n",
    "        # Estad√≠sticas\n",
    "        tau_mean = np.average(centers, weights=mean_hist)\n",
    "        tau_median = centers[np.argmax(mean_hist)]\n",
    "        skewness = ((centers - tau_mean)**3 * mean_hist).sum() / (((centers - tau_mean)**2 * mean_hist).sum())**1.5\n",
    "        \n",
    "        results.append({\n",
    "            'pair_label': row['pair_label'],\n",
    "            'n_modes': n_modes,\n",
    "            'tau_peak_ms': tau_median,\n",
    "            'tau_weighted_mean_ms': tau_mean,\n",
    "            'skewness': skewness,\n",
    "            'n_rats_kept': len(kept_rats),\n",
    "            'n_rats_total': row['n_rats'],\n",
    "            'outliers': row['n_rats'] - len(kept_rats)\n",
    "        })\n",
    "\n",
    "df_shapes = pd.DataFrame(results)\n",
    "print(\"\\nüîç AN√ÅLISIS DE FORMAS DE DISTRIBUCI√ìN (Top 15)\")\n",
    "print(\"=\"*80)\n",
    "display(df_shapes)\n",
    "\n",
    "# Clasificaci√≥n por modalidad\n",
    "print(\"\\nüìä CLASIFICACI√ìN POR MODALIDAD:\")\n",
    "print(df_shapes.groupby('n_modes').size())\n",
    "\n",
    "print(\"\\nüìà BIMODALES (n_modes ‚â• 2):\")\n",
    "display(df_shapes[df_shapes['n_modes'] >= 2][['pair_label', 'tau_peak_ms', 'skewness']])\n",
    "\n",
    "print(\"\\n‚öñÔ∏è SIMETR√çA (skewness cerca de 0 = gaussiana, >1 = cola derecha larga):\")\n",
    "display(df_shapes[['pair_label', 'skewness', 'n_modes']].sort_values('skewness'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01664cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(10):\n",
    "    \n",
    "    \n",
    "    row = df_stable_diverse.iloc[idx]\n",
    "    pair = (row['roi_i'], row['roi_j'])\n",
    "    plot_avg_distribution(pair, cleaned_rats, name_map, 50, False)\n",
    "    print(pair)\n",
    "    print(df_stable_diverse.iloc[idx]['pair_label'])\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be514fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# üß© DISTRIBUCIONES PROMEDIADAS FINALES (filtrado ¬±3œÉ)\n",
    "# ==============================================================\n",
    "\n",
    "selected_pairs = [\n",
    "    (np.int64(28), np.int64(35)),   # L-Hippocampus ‚Üí L-Hypothalamic region\n",
    "    (np.int64(108), np.int64(132)), # R-Subthalamic nucleus ‚Üí R-Retrosplenial dysgranular area\n",
    "    (np.int64(108), np.int64(137)), # R-Subthalamic nucleus ‚Üí R-Retrosplenial granular area\n",
    "    (np.int64(45), np.int64(61)),   # L-Perirhinal area 35 ‚Üí L-Endopiriform nucleus\n",
    "    (np.int64(35), np.int64(76))    # L-Hypothalamic region ‚Üí L-Medial orbital area\n",
    "]\n",
    "\n",
    "export_dir = Path('./results/data_analysis/distros/final_avg_distributions_manual')\n",
    "export_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "summary_records = []\n",
    "\n",
    "for pair in selected_pairs:\n",
    "    centers, mean_hist, kept_rats = plot_avg_distribution(\n",
    "        pair, cleaned_rats, name_map, bins=50, save=True\n",
    "    )\n",
    "\n",
    "    if centers is None:\n",
    "        continue\n",
    "\n",
    "    label = f\"{roi_label(pair[0], name_map)} ‚Üí {roi_label(pair[1], name_map)}\"\n",
    "    summary_records.append({\n",
    "        'pair': pair,\n",
    "        'label': label,\n",
    "        'n_rats_valid': len(kept_rats),\n",
    "        'kept_rats': ','.join(kept_rats),\n",
    "        'file': f\"avgdist_{roi_label(pair[0], name_map)}_to_{roi_label(pair[1], name_map)}.npz\"\n",
    "    })\n",
    "\n",
    "# Crear resumen tabular\n",
    "df_summary = pd.DataFrame(summary_records)\n",
    "df_summary.to_csv(export_dir / 'summary_avg_distributions.csv', index=False)\n",
    "\n",
    "print(\"\\n‚úÖ Exportaci√≥n completa:\")\n",
    "display(df_summary[['label', 'n_rats_valid', 'file']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1410f292",
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from scipy import stats\n",
    "\n",
    "def characterize_distributions_enhanced(cleaned_rats, pairs_list):\n",
    "    \"\"\"Features + divergencias vs referencia\"\"\"\n",
    "    rows = []\n",
    "    \n",
    "    # 1er paso: construir distribuci√≥n de referencia (pooled)\n",
    "    all_tau = []\n",
    "    for rat_id, data in cleaned_rats.items():\n",
    "        for (i, j) in pairs_list:\n",
    "            if (i, j) in data:\n",
    "                all_tau.append(data[(i,j)][:, COL_TAU] * 1e3)\n",
    "    ref_tau = np.concatenate(all_tau)\n",
    "    ref_hist, ref_edges = np.histogram(ref_tau, bins=50, density=True)\n",
    "    ref_cdf = np.cumsum(ref_hist) / ref_hist.sum()\n",
    "    \n",
    "    # 2do paso: caracterizar cada distribuci√≥n\n",
    "    for rat_id, data in cleaned_rats.items():\n",
    "        for (i, j) in pairs_list:\n",
    "            if (i, j) not in data:\n",
    "                continue\n",
    "            \n",
    "            tau_ms = data[(i,j)][:, COL_TAU] * 1e3\n",
    "            \n",
    "            # Histogram\n",
    "            hist, edges = np.histogram(tau_ms, bins='auto', density=True)\n",
    "            peaks, _ = find_peaks(hist, prominence=hist.max()*0.1)\n",
    "            \n",
    "            # M√©tricas de forma\n",
    "            g1 = stats.skew(tau_ms)\n",
    "            g2 = stats.kurtosis(tau_ms, fisher=True)\n",
    "            bimodality_coef = (g1**2 + 1) / (g2 + 3)\n",
    "            \n",
    "            hist_prob = hist / hist.sum()\n",
    "            shannon_entropy = entropy(hist_prob[hist_prob > 0])\n",
    "            \n",
    "            # Robustez\n",
    "            med = np.median(tau_ms)\n",
    "            mad = np.median(np.abs(tau_ms - med))\n",
    "            cv_robust = 1.4826 * mad / med if med > 0 else np.nan\n",
    "            \n",
    "            # Divergencias vs referencia\n",
    "            wassers_dist = wasserstein_distance(tau_ms, ref_tau)\n",
    "            ks_stat, _ = stats.ks_2samp(tau_ms, ref_tau)\n",
    "            \n",
    "            # KL divergence (discretizada, evita log(0))\n",
    "            hist_sample, _ = np.histogram(tau_ms, bins=ref_edges, density=True)\n",
    "            hist_sample = hist_sample / hist_sample.sum()\n",
    "            # A√±adir epsilon para evitar log(0)\n",
    "            eps = 1e-10\n",
    "            kl_div = entropy(hist_sample + eps, ref_hist + eps)\n",
    "            \n",
    "            rows.append({\n",
    "                'rat_id': rat_id, 'roi_i': i, 'roi_j': j,\n",
    "                'pair_label': f\"{roi_label(i, name_map)} ‚Üí {roi_label(j, name_map)}\",\n",
    "                'n': len(tau_ms),\n",
    "                'mean': tau_ms.mean(),\n",
    "                'median': med,\n",
    "                'cv_robust': cv_robust,\n",
    "                'skew': g1,\n",
    "                'kurt': g2,\n",
    "                'bimodality_coef': bimodality_coef,\n",
    "                'n_peaks': len(peaks),\n",
    "                'entropy': shannon_entropy,\n",
    "                'range_norm': np.ptp(tau_ms) / tau_ms.mean(),\n",
    "                'iqr_norm': stats.iqr(tau_ms) / tau_ms.mean(),\n",
    "                'wasserstein': wassers_dist,\n",
    "                'ks_stat': ks_stat,\n",
    "                'kl_div': kl_div\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8720b5d4",
   "metadata": {},
   "source": [
    "## Visualizaciones Multi-Rata ‚úÖ\n",
    "\n",
    "**An√°lisis morfol√≥gico**:\n",
    "\n",
    "**#1 Subthalamic‚ÜíAmygdala** (œÑ_range=4.96ms):\n",
    "- Heterog√©neo: R02/R08 unimodales estrechos vs R07/R15 dispersos\n",
    "- Medianas estables ~1.2ms pero colas largas (~8ms)\n",
    "\n",
    "**#2 Subthalamic‚ÜíRetrosplenial** (œÑ_range=4.94ms):\n",
    "- **Extremadamente consistente**: pico √∫nico ~0.7ms\n",
    "- 5000+ fibras, distribuci√≥n casi id√©ntica entre ratas\n",
    "- Candidato \"delta-like\" estable\n",
    "\n",
    "**#3 Endopiriform‚ÜíPrelimbic** (œÑ_range=4.87ms):\n",
    "- **Bimodal en varias ratas** (R01, R03, R07, R13)\n",
    "- Picos ~1ms y ~2-3ms\n",
    "- Variabilidad morfol√≥gica significativa\n",
    "\n",
    "**Conclusi√≥n visual**: Hay al menos **2-3 morfolog√≠as distintas** (unimodal estrecho, disperso, bimodal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5d3799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# üß© CLUSTERING MORFOL√ìGICO OPTIMIZADO (multi-m√©trica + grid search)\n",
    "# ==============================================================\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score,\n",
    "    davies_bouldin_score,\n",
    "    calinski_harabasz_score\n",
    ")\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 1Ô∏è‚É£ Selecci√≥n de features y preprocesamiento\n",
    "# --------------------------------------------------------------\n",
    "feature_cols = [\n",
    "    'cv_robust', 'skew', 'kurt', 'bimodality_coef',\n",
    "    'n_peaks', 'entropy', 'wasserstein', \n",
    "]\n",
    "\n",
    "# Caracterizar\n",
    "top_pairs = [(r['roi_i'], r['roi_j']) for _, r in df_stable_diverse.head(50).iterrows()]\n",
    "df_feat = characterize_distributions_enhanced(refined_rats, top_pairs)\n",
    "\n",
    "# Agregamos por conexi√≥n (media inter-rata)\n",
    "df_feat_conn = (\n",
    "    df_feat.groupby(['roi_i', 'roi_j', 'pair_label'])[feature_cols].mean()\n",
    "    .reset_index()\n",
    "    .dropna()\n",
    ")\n",
    "\n",
    "X = df_feat_conn[feature_cols].values\n",
    "X_scaled = RobustScaler().fit_transform(X)\n",
    "\n",
    "print(f\"üìä {len(df_feat_conn)} conexiones analizadas con {len(feature_cols)} features.\")\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 2Ô∏è‚É£ Grid Search: KMeans + PCA\n",
    "# --------------------------------------------------------------\n",
    "param_grid = {\n",
    "    'pca_variance': [0.7,0.8, 0.9, 0.95],\n",
    "    'n_clusters': [3,4,5,6],\n",
    "    'n_init': [50, 100]\n",
    "}\n",
    "\n",
    "results = []\n",
    "configs = list(ParameterGrid(param_grid))\n",
    "print(f\"Evaluando {len(configs)} configuraciones...\\n\")\n",
    "\n",
    "for i, params in enumerate(configs):\n",
    "    # PCA con varianza acumulada\n",
    "    pca = PCA(n_components=params['pca_variance'])\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "    # K-means\n",
    "    kmeans = KMeans(\n",
    "        n_clusters=params['n_clusters'],\n",
    "        n_init=params['n_init'],\n",
    "        random_state=42\n",
    "    )\n",
    "    labels = kmeans.fit_predict(X_pca)\n",
    "\n",
    "    # M√©tricas de calidad\n",
    "    sil = silhouette_score(X_pca, labels)\n",
    "    db = davies_bouldin_score(X_pca, labels)\n",
    "    ch = calinski_harabasz_score(X_pca, labels)\n",
    "\n",
    "    results.append({\n",
    "        **params,\n",
    "        'n_pcs': pca.n_components_,\n",
    "        'silhouette': sil,\n",
    "        'davies_bouldin': db,\n",
    "        'calinski_harabasz': ch,\n",
    "        'inertia': kmeans.inertia_\n",
    "    })\n",
    "\n",
    "    if (i + 1) % 10 == 0 or (i + 1) == len(configs):\n",
    "        print(f\"  {i + 1}/{len(configs)} completado\")\n",
    "\n",
    "df_grid = pd.DataFrame(results)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 3Ô∏è‚É£ Ponderaci√≥n multi-m√©trica normalizada\n",
    "# --------------------------------------------------------------\n",
    "scaler = MinMaxScaler()\n",
    "scaled_metrics = scaler.fit_transform(df_grid[['silhouette', 'davies_bouldin', 'calinski_harabasz']]) # TODO CUMSUM DIFF\n",
    "sil, db, ch = scaled_metrics.T\n",
    "df_grid['score'] = sil - db + ch\n",
    "\n",
    "df_grid = df_grid.sort_values('score', ascending=False)\n",
    "\n",
    "print(f\"\\n‚úì Grid search completado: {len(df_grid)} configuraciones\")\n",
    "print(\"\\nTop 5 configuraciones:\")\n",
    "display(df_grid.head(5)[['n_clusters', 'pca_variance', 'n_pcs', 'silhouette', 'davies_bouldin', 'calinski_harabasz']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6604bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# 4Ô∏è‚É£ Clustering final\n",
    "# --------------------------------------------------------------\n",
    "best = df_grid.iloc[3]\n",
    "pca_final = PCA(n_components=best['pca_variance'])\n",
    "X_pca_final = pca_final.fit_transform(X_scaled)\n",
    "\n",
    "kmeans_final = KMeans(\n",
    "    n_clusters=int(best['n_clusters']),\n",
    "    n_init=int(best['n_init']),\n",
    "    random_state=42\n",
    ")\n",
    "df_feat_conn['cluster'] = kmeans_final.fit_predict(X_pca_final)\n",
    "\n",
    "print(f\"\\nüéØ Clustering final: k={int(best['n_clusters'])}, \"\n",
    "      f\"PCs={pca_final.n_components_}, \"\n",
    "      f\"var_PCA={best['pca_variance']:.2f}\")\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 5Ô∏è‚É£ Visualizaciones resumen\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "fig1 = plt.figure(figsize=(16, 14))\n",
    "gs = fig1.add_gridspec(4, 4, hspace=0.35, wspace=0.35)\n",
    "\n",
    "# (1) Silhouette vs k\n",
    "ax1 = fig1.add_subplot(gs[0, :2])\n",
    "for pca_var in sorted(df_grid['pca_variance'].unique()):\n",
    "    subset = df_grid[(df_grid['pca_variance'] == pca_var) & (df_grid['n_init'] == 100)]\n",
    "    ax1.plot(subset['n_clusters'], subset['silhouette'], 'o-', label=f'{pca_var:.2f}', lw=2)\n",
    "ax1.set_xlabel('k (#clusters)')\n",
    "ax1.set_ylabel('Silhouette')\n",
    "ax1.legend(title='PCA var.')\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# (2) Davies-Bouldin vs k\n",
    "ax2 = fig1.add_subplot(gs[0, 2:])\n",
    "for pca_var in sorted(df_grid['pca_variance'].unique()):\n",
    "    subset = df_grid[(df_grid['pca_variance'] == pca_var) & (df_grid['n_init'] == 100)]\n",
    "    ax2.plot(subset['n_clusters'], subset['davies_bouldin'], 'o-', label=f'{pca_var:.2f}', lw=2)\n",
    "ax2.set_xlabel('k (#clusters)')\n",
    "ax2.set_ylabel('Davies-Bouldin ‚Üì')\n",
    "ax2.legend(title='PCA var.')\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "# (3) Calinski-Harabasz medio por varianza PCA\n",
    "ax3 = fig1.add_subplot(gs[1, :2])\n",
    "grouped = df_grid.groupby('pca_variance')['calinski_harabasz'].mean()\n",
    "ax3.bar(grouped.index, grouped.values, alpha=0.7, edgecolor='k')\n",
    "ax3.set_xlabel('PCA variance')\n",
    "ax3.set_ylabel('Calinski-Harabasz')\n",
    "ax3.grid(alpha=0.3, axis='y')\n",
    "\n",
    "# (4) PCA final scatter\n",
    "ax4 = fig1.add_subplot(gs[1, 2:])\n",
    "sc = ax4.scatter(\n",
    "    X_pca_final[:, 0], X_pca_final[:, 1],\n",
    "    c=df_feat_conn['cluster'], cmap='tab10',\n",
    "    s=60, alpha=0.8, edgecolors='k', lw=0.4\n",
    ")\n",
    "ax4.set_xlabel(f'PC1 ({pca_final.explained_variance_ratio_[0]:.1%})')\n",
    "ax4.set_ylabel(f'PC2 ({pca_final.explained_variance_ratio_[1]:.1%})')\n",
    "ax4.set_title(f\"PCA (k={int(best['n_clusters'])}, var={best['pca_variance']:.2f})\")\n",
    "plt.colorbar(sc, ax=ax4, label='Cluster')\n",
    "plt.suptitle('K-means + PCA (Optimized)', fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 6Ô∏è‚É£ Heatmaps de correlaciones y perfiles\n",
    "# --------------------------------------------------------------\n",
    "fig2, ax = plt.subplots(figsize=(10, 8))\n",
    "corr_cols = ['n_clusters', 'pca_variance', 'n_init', 'silhouette', 'davies_bouldin', 'calinski_harabasz', 'score']\n",
    "sns.heatmap(df_grid[corr_cols].corr(), annot=True, fmt='.2f', cmap='RdBu_r', center=0, ax=ax, square=True)\n",
    "ax.set_title('Correlaciones entre par√°metros y m√©tricas')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Perfil morfol√≥gico por cluster\n",
    "fig3, ax = plt.subplots(figsize=(10, 6))\n",
    "cluster_profiles = df_feat_conn.groupby('cluster')[feature_cols].median()\n",
    "sns.heatmap(cluster_profiles.T, annot=True, fmt='.2f', cmap='coolwarm', center=0, ax=ax)\n",
    "ax.set_title('Perfil morfol√≥gico por cluster')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 7Ô∏è‚É£ Estad√≠sticas finales\n",
    "# --------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CLUSTERING FINAL ‚Äî RESUMEN\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Par√°metros √≥ptimos:\")\n",
    "print(f\"  k = {int(best['n_clusters'])}\")\n",
    "print(f\"  PCA var = {best['pca_variance']:.2f} ({pca_final.n_components_} componentes)\")\n",
    "print(\"\\nM√©tricas:\")\n",
    "print(f\"  Silhouette         : {best['silhouette']:.3f}\")\n",
    "print(f\"  Davies-Bouldin     : {best['davies_bouldin']:.3f}\")\n",
    "print(f\"  Calinski-Harabasz  : {best['calinski_harabasz']:.1f}\")\n",
    "print(\"\\nDistribuci√≥n de clusters:\")\n",
    "print(df_feat_conn['cluster'].value_counts().sort_index())\n",
    "print(\"=\"*70)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 8Ô∏è‚É£ Guardado de resultados\n",
    "# --------------------------------------------------------------\n",
    "threshold = 0.0  # o 0.0 / 0.4 seg√∫n el dataset\n",
    "df_grid.to_csv(f'gridsearch_kmeans_th_{threshold}.csv', index=False)\n",
    "df_feat_conn.to_csv(f'feature_clusters_th_{threshold}.csv', index=False)\n",
    "print(f\"\\n‚úÖ Resultados guardados para threshold={threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a17c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dae1d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat_conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead90dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplos por cluster\n",
    "np.random.seed(42)\n",
    "\n",
    "for c in sorted(df_feat_conn['cluster'].unique()):\n",
    "    # Seleccionar 2 conexiones del cluster\n",
    "    cluster_conns = df_feat_conn[df_feat_conn['cluster'] == c].head(2)\n",
    "    \n",
    "    if len(cluster_conns) == 0:\n",
    "        continue\n",
    "    \n",
    "    n_plots = len(cluster_conns)\n",
    "    fig, axes = plt.subplots(1, n_plots, figsize=(6*n_plots, 4))\n",
    "    if n_plots == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    cluster_profile = df_feat_conn[df_feat_conn['cluster'] == c][feature_cols].median()\n",
    "    \n",
    "    for idx, (_, row) in enumerate(cluster_conns.iterrows()):\n",
    "        i, j = int(row['roi_i']), int(row['roi_j'])\n",
    "        \n",
    "        # Tomar primera rata disponible para esta conexi√≥n\n",
    "        rat_id = None\n",
    "        for rid, data in cleaned_rats.items():\n",
    "            if (i, j) in data:\n",
    "                rat_id = rid\n",
    "                break\n",
    "        \n",
    "        if rat_id is None:\n",
    "            continue\n",
    "            \n",
    "        tau_ms = cleaned_rats[rat_id][(i,j)][:, COL_TAU] * 1e3\n",
    "        \n",
    "        axes[idx].hist(tau_ms, bins=40, alpha=0.75, edgecolor='k', linewidth=0.5)\n",
    "        axes[idx].axvline(np.median(tau_ms), color='r', ls='--', lw=2)\n",
    "        axes[idx].set_title(f\"{row['pair_label'][:50]}\\n({rat_id}, n={len(tau_ms)})\", fontsize=9)\n",
    "        axes[idx].set_xlabel('œÑ (ms)')\n",
    "        axes[idx].set_ylabel('Count')\n",
    "    \n",
    "    fig.suptitle(\n",
    "        f'Cluster {c} | CV={cluster_profile[\"cv_robust\"]:.2f}, '\n",
    "        f'skew={cluster_profile[\"skew\"]:.2f}, kurt={cluster_profile[\"kurt\"]:.2f}',\n",
    "        fontsize=12\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6d4397",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Caracterizar\n",
    "top_pairs = [(r['roi_i'], r['roi_j']) for _, r in df_stable_diverse.head(50).iterrows()]\n",
    "df_feat = characterize_distributions_enhanced(cleaned_rats, top_pairs)\n",
    "\n",
    "# Features para clustering (robustos a outliers)\n",
    "X = df_feat[['cv_robust', 'skew', 'bimodality_coef', 'iqr_norm', 'wasserstein', 'ks_stat', 'mean', 'median', 'kurt', 'n_peaks', 'entropy', 'kl_div', 'range_norm']].values\n",
    "\n",
    "feature_cols = ['cv_robust', 'skew', 'kurt', 'bimodality_coef', \n",
    "                'n_peaks', 'entropy', 'wasserstein']\n",
    "\n",
    "corr_cols = ['n_neighbors', 'min_dist', 'min_cluster_size', 'min_samples',\n",
    "             'silhouette', 'davies_bouldin', 'n_clusters', 'noise_pct']\n",
    "\n",
    "X = df_feat[feature_cols].dropna().values\n",
    "X_scaled = RobustScaler().fit_transform(X)\n",
    "\n",
    "# ===== GRID SEARCH =====\n",
    "param_grid = {\n",
    "    'n_neighbors': [10, 20],\n",
    "    'min_dist': [0.0, 0.2],\n",
    "    'min_cluster_size': [10, 100],  # m√°s grande ‚Üí menos clusters\n",
    "    'min_samples': [5, 10]\n",
    "}\n",
    "\n",
    "results = []\n",
    "print(f\"Evaluando {len(list(ParameterGrid(param_grid)))} configuraciones...\")\n",
    "\n",
    "for i, params in enumerate(ParameterGrid(param_grid)):\n",
    "    # UMAP\n",
    "    reducer = UMAP(n_neighbors=params['n_neighbors'], \n",
    "                   min_dist=params['min_dist'],\n",
    "                   n_components=2, n_jobs=-1)\n",
    "    X_umap = reducer.fit_transform(X_scaled)\n",
    "    \n",
    "    # HDBSCAN\n",
    "    clusterer = HDBSCAN(min_cluster_size=params['min_cluster_size'],\n",
    "                        min_samples=params['min_samples'], \n",
    "                        core_dist_n_jobs=-1)\n",
    "    labels = clusterer.fit_predict(X_umap)\n",
    "    \n",
    "    # M√©tricas (solo no-ruido)\n",
    "    mask = labels != -1\n",
    "    n_noise = (~mask).sum()\n",
    "    n_clustered = mask.sum()\n",
    "    n_clusters = len(set(labels[mask])) if n_clustered > 0 else 0\n",
    "    \n",
    "    if n_clustered > 20 and n_clusters > 1:\n",
    "        sil = silhouette_score(X_umap[mask], labels[mask])\n",
    "        db = davies_bouldin_score(X_umap[mask], labels[mask])\n",
    "        ch = calinski_harabasz_score(X_umap[mask], labels[mask])\n",
    "        noise_pct = 100 * n_noise / len(labels)\n",
    "        \n",
    "        results.append({\n",
    "            **params, \n",
    "            'silhouette': sil, \n",
    "            'davies_bouldin': db,\n",
    "            'calinski_harabasz': ch,\n",
    "            'n_clusters': n_clusters, \n",
    "            'noise_pct': noise_pct,\n",
    "            'n_clustered': n_clustered\n",
    "        })\n",
    "    \n",
    "    if (i+1) % 20 == 0:\n",
    "        print(f\"  {i+1}/{len(list(ParameterGrid(param_grid)))} completado\")\n",
    "\n",
    "df_grid = pd.DataFrame(results)\n",
    "\n",
    "# Mejor configuraci√≥n (multiobjetivo)\n",
    "# Cambiar funci√≥n de score\n",
    "df_grid['score'] = (\n",
    "    df_grid['silhouette'] / df_grid['silhouette'].max() - \n",
    "    df_grid['davies_bouldin'] / df_grid['davies_bouldin'].max() +\n",
    "    df_grid['calinski_harabasz'] / df_grid['calinski_harabasz'].max() -\n",
    "    df_grid['noise_pct'] / 100 -\n",
    "    abs(df_grid['n_clusters'] - 4) / 10  # penaliza alejarse de k=4\n",
    ")\n",
    "df_grid = df_grid.sort_values('score', ascending=False)\n",
    "\n",
    "print(f\"\\n‚úì Grid search completo: {len(df_grid)} configuraciones v√°lidas\")\n",
    "print(\"\\nTop 5 configuraciones:\")\n",
    "print(df_grid.head(5)[['n_neighbors', 'min_dist', 'min_cluster_size', 'min_samples', \n",
    "                        'n_clusters', 'silhouette', 'noise_pct']].to_string(index=False))\n",
    "\n",
    "# ===== CLUSTERING FINAL =====\n",
    "best = df_grid.iloc[0]\n",
    "reducer_final = UMAP(n_neighbors=int(best['n_neighbors']), \n",
    "                     min_dist=best['min_dist'],\n",
    "                     n_components=2, random_state=42)\n",
    "X_umap_final = reducer_final.fit_transform(X_scaled)\n",
    "\n",
    "clusterer_final = HDBSCAN(min_cluster_size=int(best['min_cluster_size']),\n",
    "                          min_samples=int(best['min_samples']))\n",
    "df_feat['cluster'] = clusterer_final.fit_predict(X_umap_final)\n",
    "\n",
    "mask_final = df_feat['cluster'] != -1\n",
    "print(f\"\\nClustering final: {df_feat['cluster'].nunique()-1} clusters + {(~mask_final).sum()} noise\")\n",
    "\n",
    "# ===== PLOTS PRINCIPALES (4x4) =====\n",
    "fig1 = plt.figure(figsize=(16, 14))\n",
    "gs = fig1.add_gridspec(4, 4, hspace=0.35, wspace=0.35)\n",
    "\n",
    "# Fila 1: M√©tricas vs par√°metros\n",
    "ax1 = fig1.add_subplot(gs[0, :2])\n",
    "for metric in ['silhouette', 'davies_bouldin']:\n",
    "    grouped = df_grid.groupby('n_neighbors')[metric].mean()\n",
    "    ax1.plot(grouped.index, grouped.values, 'o-', label=metric, lw=2)\n",
    "ax1.set_xlabel('n_neighbors'); ax1.legend(); ax1.grid(alpha=0.3)\n",
    "\n",
    "ax2 = fig1.add_subplot(gs[0, 2:])\n",
    "grouped = df_grid.groupby('min_cluster_size')['noise_pct'].mean()\n",
    "ax2.bar(grouped.index, grouped.values, alpha=0.7)\n",
    "ax2.set_xlabel('min_cluster_size'); ax2.set_ylabel('% Noise')\n",
    "\n",
    "# Fila 2: Scatter n_clusters\n",
    "ax3 = fig1.add_subplot(gs[1, :])\n",
    "for md in sorted(df_grid['min_dist'].unique()):\n",
    "    subset = df_grid[df_grid['min_dist'] == md]\n",
    "    ax3.scatter(subset['n_neighbors'], subset['n_clusters'], \n",
    "               label=f'min_dist={md}', s=40)\n",
    "ax3.legend(); ax3.grid(alpha=0.3)\n",
    "\n",
    "# Fila 3-4: UMAP grande\n",
    "ax4 = fig1.add_subplot(gs[2:, :])\n",
    "scatter = ax4.scatter(X_umap_final[:, 0], X_umap_final[:, 1], \n",
    "                     c=df_feat['cluster'], cmap='tab10', s=60, alpha=0.7)\n",
    "ax4.set_xlabel('UMAP 1'); ax4.set_ylabel('UMAP 2')\n",
    "plt.colorbar(scatter, ax=ax4)\n",
    "\n",
    "plt.suptitle(f'UMAP+HDBSCAN Optimization (th=0.0)', fontsize=14)\n",
    "plt.savefig(f'clustering_main_th_0.0.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ===== HEATMAPS SEPARADOS =====\n",
    "# Correlaciones\n",
    "fig2, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "corr_matrix = df_grid[corr_cols].corr()\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='RdBu_r', \n",
    "            center=0, ax=ax, square=True)\n",
    "ax.set_title('Correlaciones')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'correlations_th_0.0.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Perfil clusters\n",
    "fig3, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "cluster_profiles = df_feat[mask_final].groupby('cluster')[feature_cols].median()\n",
    "sns.heatmap(cluster_profiles.T, annot=True, fmt='.2f', cmap='coolwarm',\n",
    "            center=0, ax=ax)\n",
    "ax.set_title('Perfil Morfol√≥gico')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'profiles_th_0.0.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# ===== ESTAD√çSTICAS FINALES =====\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ESTAD√çSTICAS CLUSTERING FINAL\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Par√°metros √≥ptimos:\")\n",
    "print(f\"  n_neighbors={int(best['n_neighbors'])}, min_dist={best['min_dist']}\")\n",
    "print(f\"  min_cluster_size={int(best['min_cluster_size'])}, min_samples={int(best['min_samples'])}\")\n",
    "print(f\"\\nM√©tricas:\")\n",
    "print(f\"  Silhouette: {best['silhouette']:.3f}\")\n",
    "print(f\"  Davies-Bouldin: {best['davies_bouldin']:.3f}\")\n",
    "print(f\"  Calinski-Harabasz: {best['calinski_harabasz']:.1f}\")\n",
    "print(f\"  Noise: {best['noise_pct']:.1f}%\")\n",
    "print(f\"\\nDistribuci√≥n por cluster:\")\n",
    "print(df_feat['cluster'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833d6430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 ejemplos por cluster\n",
    "np.random.seed(42)\n",
    "for c in sorted(df_feat['cluster'].unique()):\n",
    "    # Tomar 2 pares distintos del cluster\n",
    "    cluster_pairs = df_feat[df_feat['cluster'] == c][['roi_i', 'roi_j', 'rat_id']].drop_duplicates(['roi_i', 'roi_j']).head(2)\n",
    "    \n",
    "    if len(cluster_pairs) == 0:\n",
    "        continue\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    cluster_profile = df_feat[df_feat['cluster'] == c][['cv_robust', 'skew', 'n_peaks']].median()\n",
    "    \n",
    "    for idx, (_, row) in enumerate(cluster_pairs.iterrows()):\n",
    "        i, j, rat = int(row['roi_i']), int(row['roi_j']), row['rat_id']\n",
    "        tau_ms = cleaned_rats[rat][(i,j)][:, COL_TAU] * 1e3\n",
    "        \n",
    "        axes[idx].hist(tau_ms, bins=40, alpha=0.75, edgecolor='k', linewidth=0.5)\n",
    "        axes[idx].axvline(np.median(tau_ms), color='r', ls='--', lw=2)\n",
    "        axes[idx].set_title(f\"{roi_label(i, name_map)[:25]}\\n‚Üí {roi_label(j, name_map)[:25]}\", fontsize=9)\n",
    "        axes[idx].set_xlabel('œÑ (ms)')\n",
    "    \n",
    "    fig.suptitle(f'Cluster {c} | CV={cluster_profile[\"cv_robust\"]:.2f}, skew={cluster_profile[\"skew\"]:.2f}, peaks={cluster_profile[\"n_peaks\"]:.0f}', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
