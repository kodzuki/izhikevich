{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a828f359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /home/ubuntu-deduce/Projects/izhikevich\n",
      "Python path: /home/ubuntu-deduce/.asdf/installs/python/3.10.13/lib/python310.zip\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# NOTEBOOK SETUP - Two Population Delay Sweep Execution\n",
    "# =============================================================================\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Robust directory setup\n",
    "if Path.cwd().name == 'two_populations':\n",
    "    os.chdir(\"../../\")\n",
    "\n",
    "print(f\"Working directory: {Path.cwd()}\")\n",
    "print(f\"Python path: {sys.path[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3051f971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "INSPECTING: ./results/experiments/two_populations/step_input_diff_rois/20251006_172926/MS_LOGNORMAL_SEMISTRONG_k3p5_dt0p1_T4000_20trials__/figures/MS_SEMISTRONG_step_input_lognorm_1p1-0p3/trial_1_full.pkl\n",
      "============================================================\n",
      "✓ Pickle loaded successfully\n",
      "\n",
      "--- KEYS IN PICKLE ---\n",
      "Total keys: 17\n",
      "  autocorr_A           → dict with 5 keys: ['lags', 'correlation', 'peak_lag', 'peak_value', 'lag_sign_convention']\n",
      "  autocorr_B           → dict with 5 keys: ['lags', 'correlation', 'peak_lag', 'peak_value', 'lag_sign_convention']\n",
      "  coherence            → dict with 9 keys: ['freqs', 'coherence', 'peak_freq', 'peak_coherence', 'theta_coherence']\n",
      "  cross_correlation    → dict with 5 keys: ['lags', 'correlation', 'peak_lag', 'peak_value', 'lag_sign_convention']\n",
      "  int_A                → dict with 3 keys: ['tau', 'quality', 'fit_quality']\n",
      "  int_B                → dict with 3 keys: ['tau', 'quality', 'fit_quality']\n",
      "  plv_pli              → dict with 5 keys: ['theta', 'alpha', 'beta', 'gamma', 'broadband']\n",
      "  psd_A                → dict with 7 keys: ['freqs', 'psd', 'alpha_power', 'gamma_power', 'total_power']\n",
      "  psd_B                → dict with 7 keys: ['freqs', 'psd', 'alpha_power', 'gamma_power', 'total_power']\n",
      "  rate_A               → ndarray (16000,) float64 (0.12 MB) [15971 non-zero]\n",
      "      Range: [0.000, 52.800], mean=10.474\n",
      "  rate_B               → ndarray (16000,) float64 (0.12 MB) [15983 non-zero]\n",
      "      Range: [0.000, 69.600], mean=9.977\n",
      "  spike_neurons_A      → ndarray (41896,) int32 (0.16 MB) [41852 non-zero]\n",
      "      Range: [0.000, 999.000], mean=499.703\n",
      "  spike_neurons_B      → ndarray (39909,) int32 (0.15 MB) [39881 non-zero]\n",
      "      Range: [0.000, 999.000], mean=531.415\n",
      "  spike_times_A        → ndarray (41896,) float64 (0.32 MB) [41896 non-zero]\n",
      "      Range: [1.900, 3999.900], mean=1982.615\n",
      "  spike_times_B        → ndarray (39909,) float64 (0.30 MB) [39909 non-zero]\n",
      "      Range: [2.000, 3999.900], mean=1980.947\n",
      "  t0_ms                → float = 500.0\n",
      "  time                 → ndarray (14000,) float64 (0.11 MB) [13999 non-zero]\n",
      "      Range: [0.000, 3499.750], mean=1749.875\n",
      "\n",
      "--- CRITICAL KEYS FOR PLOTTING ---\n",
      "  rate_A               ✓ OK\n",
      "  rate_B               ✓ OK\n",
      "  time                 ✓ OK\n",
      "  spike_times_A        ✓ OK\n",
      "  spike_times_B        ✓ OK\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from typing import Dict, List\n",
    "\n",
    "def inspect_pickle_structure(sweep_dir: str, config_name: str, trial_idx: int = 0):\n",
    "    \"\"\"Inspeccionar contenido de un pickle específico\"\"\"\n",
    "    \n",
    "    config_dir = os.path.join(sweep_dir, \"figures\", config_name)\n",
    "    trial_file = os.path.join(config_dir, f\"trial_{trial_idx+1}_full.pkl\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"INSPECTING: {trial_file}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    if not os.path.exists(trial_file):\n",
    "        print(f\"❌ FILE NOT FOUND: {trial_file}\")\n",
    "        print(f\"   Directory exists: {os.path.exists(config_dir)}\")\n",
    "        if os.path.exists(config_dir):\n",
    "            files = os.listdir(config_dir)\n",
    "            print(f\"   Available files: {files[:10]}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        with open(trial_file, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "        print(f\"✓ Pickle loaded successfully\")\n",
    "        print(f\"\\n--- KEYS IN PICKLE ---\")\n",
    "        print(f\"Total keys: {len(data.keys())}\")\n",
    "        \n",
    "        for key in sorted(data.keys()):\n",
    "            value = data[key]\n",
    "            \n",
    "            # Analizar tipo y tamaño\n",
    "            if isinstance(value, np.ndarray):\n",
    "                shape = value.shape\n",
    "                dtype = value.dtype\n",
    "                size_mb = value.nbytes / (1024**2)\n",
    "                non_zero = np.count_nonzero(value)\n",
    "                print(f\"  {key:20s} → ndarray {shape} {dtype} ({size_mb:.2f} MB) [{non_zero} non-zero]\")\n",
    "                \n",
    "                # Mostrar preview si es 1D pequeño\n",
    "                if len(shape) == 1 and shape[0] < 10:\n",
    "                    print(f\"      Preview: {value}\")\n",
    "                elif len(shape) == 1 and shape[0] > 0:\n",
    "                    print(f\"      Range: [{value.min():.3f}, {value.max():.3f}], mean={value.mean():.3f}\")\n",
    "                    \n",
    "            elif isinstance(value, dict):\n",
    "                print(f\"  {key:20s} → dict with {len(value)} keys: {list(value.keys())[:5]}\")\n",
    "                \n",
    "            elif isinstance(value, (int, float)):\n",
    "                print(f\"  {key:20s} → {type(value).__name__} = {value}\")\n",
    "                \n",
    "            else:\n",
    "                print(f\"  {key:20s} → {type(value).__name__}\")\n",
    "        \n",
    "        # Verificar claves críticas para plotting\n",
    "        print(f\"\\n--- CRITICAL KEYS FOR PLOTTING ---\")\n",
    "        critical_keys = ['rate_A', 'rate_B', 'time', 'spike_times_A', 'spike_times_B']\n",
    "        \n",
    "        for key in critical_keys:\n",
    "            if key in data:\n",
    "                value = data[key]\n",
    "                if isinstance(value, np.ndarray):\n",
    "                    status = \"✓ OK\" if len(value) > 100 else f\"⚠ SHORT ({len(value)} elements)\"\n",
    "                    print(f\"  {key:20s} {status}\")\n",
    "                else:\n",
    "                    print(f\"  {key:20s} ❌ NOT AN ARRAY (type: {type(value).__name__})\")\n",
    "            else:\n",
    "                print(f\"  {key:20s} ❌ MISSING\")\n",
    "        \n",
    "        return data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ ERROR LOADING PICKLE: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "\n",
    "def diagnose_all_configs(sweep_dir: str, results_db: Dict):\n",
    "    \"\"\"Diagnosticar todos los configs del sweep\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"DIAGNOSING ALL CONFIGS IN: {sweep_dir}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    configs = list(results_db.keys())\n",
    "    print(f\"\\nTotal configs: {len(configs)}\")\n",
    "    \n",
    "    summary = {\n",
    "        'configs_with_pickles': 0,\n",
    "        'configs_without_pickles': 0,\n",
    "        'pickles_with_data': 0,\n",
    "        'pickles_empty': 0,\n",
    "        'missing_rate_A': [],\n",
    "        'missing_rate_B': [],\n",
    "        'short_timeseries': []\n",
    "    }\n",
    "    \n",
    "    for config_name in configs:\n",
    "        config_dir = os.path.join(sweep_dir, \"figures\", config_name)\n",
    "        trial_file = os.path.join(config_dir, \"trial_1_full.pkl\")\n",
    "        \n",
    "        if not os.path.exists(trial_file):\n",
    "            summary['configs_without_pickles'] += 1\n",
    "            print(f\"  ❌ {config_name}: no pickle found\")\n",
    "            continue\n",
    "        \n",
    "        summary['configs_with_pickles'] += 1\n",
    "        \n",
    "        try:\n",
    "            with open(trial_file, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "            \n",
    "            # Check critical keys\n",
    "            has_rate_A = 'rate_A' in data\n",
    "            has_rate_B = 'rate_B' in data\n",
    "            \n",
    "            if not has_rate_A:\n",
    "                summary['missing_rate_A'].append(config_name)\n",
    "            if not has_rate_B:\n",
    "                summary['missing_rate_B'].append(config_name)\n",
    "            \n",
    "            if has_rate_A and has_rate_B:\n",
    "                len_A = len(data['rate_A']) if isinstance(data['rate_A'], np.ndarray) else 0\n",
    "                len_B = len(data['rate_B']) if isinstance(data['rate_B'], np.ndarray) else 0\n",
    "                \n",
    "                if len_A > 100 and len_B > 100:\n",
    "                    summary['pickles_with_data'] += 1\n",
    "                    status = \"✓\"\n",
    "                elif len_A == 0 or len_B == 0:\n",
    "                    summary['pickles_empty'] += 1\n",
    "                    status = \"❌ EMPTY\"\n",
    "                else:\n",
    "                    summary['short_timeseries'].append(config_name)\n",
    "                    status = f\"⚠ SHORT (A={len_A}, B={len_B})\"\n",
    "                    \n",
    "                print(f\"  {status} {config_name}\")\n",
    "            else:\n",
    "                summary['pickles_empty'] += 1\n",
    "                print(f\"  ❌ INCOMPLETE {config_name}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ ERROR {config_name}: {e}\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"SUMMARY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Configs with pickles:     {summary['configs_with_pickles']}/{len(configs)}\")\n",
    "    print(f\"Configs without pickles:  {summary['configs_without_pickles']}/{len(configs)}\")\n",
    "    print(f\"Pickles with data (>100): {summary['pickles_with_data']}\")\n",
    "    print(f\"Pickles empty:            {summary['pickles_empty']}\")\n",
    "    print(f\"Pickles with short data:  {len(summary['short_timeseries'])}\")\n",
    "    \n",
    "    if summary['missing_rate_A']:\n",
    "        print(f\"\\nConfigs missing rate_A: {summary['missing_rate_A']}\")\n",
    "    if summary['missing_rate_B']:\n",
    "        print(f\"Configs missing rate_B: {summary['missing_rate_B']}\")\n",
    "    if summary['short_timeseries']:\n",
    "        print(f\"Configs with short timeseries: {summary['short_timeseries']}\")\n",
    "    \n",
    "    return summary\n",
    "\n",
    "\n",
    "def fix_load_raw_timeseries_with_logging():\n",
    "    \"\"\"Versión corregida con logging detallado\"\"\"\n",
    "    \n",
    "    code = '''\n",
    "def load_raw_timeseries(sweep_dir, config_name, trial_idx=0):\n",
    "    \"\"\"Cargar datos raw con debugging mejorado\"\"\"\n",
    "    \n",
    "    config_dir = os.path.join(sweep_dir, \"figures\", config_name)\n",
    "    trial_file = os.path.join(config_dir, f\"trial_{trial_idx+1}_full.pkl\")\n",
    "    \n",
    "    logger.info(f\"[LOAD] Buscando: {trial_file}\")\n",
    "    \n",
    "    if not os.path.exists(trial_file):\n",
    "        logger.warning(f\"[LOAD] ❌ No encontrado: {trial_file}\")\n",
    "        logger.warning(f\"[LOAD]    Dir exists: {os.path.exists(config_dir)}\")\n",
    "        if os.path.exists(config_dir):\n",
    "            files = [f for f in os.listdir(config_dir) if 'trial' in f]\n",
    "            logger.warning(f\"[LOAD]    Available trial files: {files[:5]}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        with open(trial_file, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "        logger.info(f\"[LOAD] ✓ Pickle cargado: {len(data)} keys\")\n",
    "        \n",
    "        # Verificar claves críticas\n",
    "        critical_keys = ['rate_A', 'rate_B', 'time']\n",
    "        missing = [k for k in critical_keys if k not in data]\n",
    "        \n",
    "        if missing:\n",
    "            logger.warning(f\"[LOAD] ⚠ Faltan claves: {missing}\")\n",
    "            logger.warning(f\"[LOAD]    Claves disponibles: {list(data.keys())[:15]}\")\n",
    "            return None\n",
    "        \n",
    "        # Verificar longitud\n",
    "        len_A = len(data['rate_A']) if isinstance(data['rate_A'], np.ndarray) else 0\n",
    "        len_B = len(data['rate_B']) if isinstance(data['rate_B'], np.ndarray) else 0\n",
    "        len_t = len(data['time']) if isinstance(data['time'], np.ndarray) else 0\n",
    "        \n",
    "        logger.info(f\"[LOAD]    rate_A: {len_A} points, rate_B: {len_B} points, time: {len_t} points\")\n",
    "        \n",
    "        if len_A < 100 or len_B < 100:\n",
    "            logger.warning(f\"[LOAD] ⚠ Time series muy cortas: A={len_A}, B={len_B}\")\n",
    "            return None\n",
    "        \n",
    "        logger.info(f\"[LOAD] ✓ Datos válidos cargados\")\n",
    "        return data\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"[LOAD] ❌ Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "    '''\n",
    "    \n",
    "    return code\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# SCRIPT DE USO\n",
    "# ===================================================================\n",
    "\n",
    "\n",
    "# Uso ejemplo:\n",
    "\n",
    "# 1. Diagnosticar un pickle específico\n",
    "sweep_dir = \"./results/experiments/two_populations/step_input_diff_rois/20251006_172926/MS_LOGNORMAL_SEMISTRONG_k3p5_dt0p1_T4000_20trials__\"  # TU PATH\n",
    "config_name = \"MS_SEMISTRONG_step_input_lognorm_1p1-0p3\"  # TU CONFIG\n",
    "\n",
    "data = inspect_pickle_structure(sweep_dir, config_name, trial_idx=0)\n",
    "\n",
    "# 2. Diagnosticar todos los configs\n",
    "# results_db = load_results_from_csvs(sweep_dir, config_names)\n",
    "# summary = diagnose_all_configs(sweep_dir, results_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a23bca81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available CPU cores: 12\n",
      "[DEVICE] runtime mode (single-threaded)\n",
      "Brian2 device: <brian2.devices.device.RuntimeDevice object at 0x7f951ffaddb0>\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BRIAN2 SETUP\n",
    "# =============================================================================\n",
    "from brian2 import *\n",
    "\n",
    "# Performance optimization for standalone compilation\n",
    "os.environ['CXXFLAGS'] = '-fopenmp -O3'\n",
    "os.environ['LDFLAGS'] = '-lgomp'\n",
    "\n",
    "# Clean slate\n",
    "restore_initial_state()\n",
    "\n",
    "# =============================================================================\n",
    "# DEVICE CONFIGURATION\n",
    "# =============================================================================\n",
    "# Options: 'runtime', 'cpp_standalone', 'cuda_standalone'\n",
    "DEVICE_MODE = 'runtime'  # Change this for performance\n",
    "\n",
    "# Hardware detection\n",
    "import multiprocessing\n",
    "N_CORES = multiprocessing.cpu_count()\n",
    "print(f\"Available CPU cores: {N_CORES}\")\n",
    "\n",
    "if DEVICE_MODE == 'cpp_standalone':\n",
    "    set_device('cpp_standalone', directory='brian2_output', build_on_run=True)\n",
    "    prefs.devices.cpp_standalone.openmp_threads = min(N_CORES, 20)  # Adaptive\n",
    "    print(f\"[DEVICE] cpp_standalone with {prefs.devices.cpp_standalone.openmp_threads} threads\")\n",
    "\n",
    "elif DEVICE_MODE == 'cuda_standalone':\n",
    "    try:\n",
    "        # from brian2cuda import *  # Uncomment if available\n",
    "        set_device('cuda_standalone', directory='brian2_cuda_output', build_on_run=False)\n",
    "        print(\"[DEVICE] cuda_standalone enabled\")\n",
    "    except ImportError:\n",
    "        print(\"[WARNING] brian2cuda not available, falling back to cpp_standalone\")\n",
    "        set_device('cpp_standalone', directory='brian2_output', build_on_run=True)\n",
    "        \n",
    "else:  # runtime\n",
    "    print(f\"[DEVICE] runtime mode (single-threaded)\")\n",
    "\n",
    "print(f\"Brian2 device: {get_device()}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f13abb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.two_populations.plots.basic_plots import plot_raster_results\n",
    "from src.two_populations.model import IzhikevichNetwork\n",
    "from src.two_populations.metrics import analyze_simulation_results\n",
    "from src.two_populations.helpers.helpers import load_results_from_csvs, extract_delay_range_from_config, load_raw_timeseries\n",
    "\n",
    "# ==== CREAR CARPETA DEL SWEEP Y GUARDAR CONFIG ====\n",
    "from src.two_populations.sweep import (\n",
    "    create_sweep_folder, save_config_json, delay_distribution_sweep,\n",
    "    save_results_with_csv, save_results\n",
    ")\n",
    "\n",
    "# ==== CONFIG BÁSICA DEL SWEEP ====\n",
    "from datetime import datetime\n",
    "import os, json\n",
    "from copy import deepcopy\n",
    "import multiprocessing\n",
    "\n",
    "from src.two_populations.plots.plots_sweeps import plot_delay_distribution_comparison\n",
    "\n",
    "def run_single_config(args):\n",
    "    config_name, delay_config, sweep_dir, dt, time, base_params, n_trials, device_mode, directionality = args\n",
    "    single_config = {config_name: delay_config}\n",
    "    \n",
    "    print(f\"Config params: {args=}\")\n",
    "    \n",
    "    results_db = delay_distribution_sweep(\n",
    "        sweep_dir, dt, time, IzhikevichNetwork,\n",
    "        base_params, single_config, \n",
    "        directionality=directionality,\n",
    "        n_trials=n_trials, \n",
    "        device_mode=device_mode\n",
    "    )\n",
    "    \n",
    "    paths = save_results_with_csv(\n",
    "        results_db, \n",
    "        out_dir=os.path.join(sweep_dir, f\"config_{config_name}\"),\n",
    "        tag=config_name,\n",
    "        per_trial_csv=True\n",
    "    )\n",
    "    \n",
    "    return config_name\n",
    "\n",
    "names = ['WEAK', 'MEDIUM', 'SEMISTRONG', 'STRONG']\n",
    "values = [0.2, 0.5, 1.0, 2.0]\n",
    "\n",
    "for i in range(0,1):\n",
    "    \n",
    "    TAG = f\"MS_BETA_MULTI_SAME_NOISE_CI_STEP_{names[i]}_{str(values[i]).replace('.','p')}_k3p5_dt0p1_T4000_30trials\"\n",
    "    DT_MS = 0.1\n",
    "    T_MS  = 4000\n",
    "    N_TRIALS = 30\n",
    "    DEVICE_MODE = \"runtime\"\n",
    "    DIRECTIONALITY = 0\n",
    "\n",
    "    # tus params (tal cual los defines)\n",
    "    k_factor = 3.5\n",
    "    noise_int = 5.1*1.2\n",
    "    noise_inh_factor = 0.451\n",
    "    inter_k_factor = values[i]\n",
    "\n",
    "    base_params = {\n",
    "        'pop_A': {'Ne': 800,'Ni': 200,'k_exc': k_factor*0.5,'k_inh': k_factor*1.0,\n",
    "                'noise_exc': noise_int,'noise_inh': round(noise_int*noise_inh_factor, 2),\n",
    "                'p_intra': 0.1,'delay': 0.2,'noise_type': 'poisson', 'step': True},\n",
    "        'pop_B': {'Ne': 800,'Ni': 200,'k_exc': k_factor*0.5,'k_inh': k_factor*1.0,\n",
    "                'noise_exc': noise_int*0.8,'noise_inh': round(noise_int*noise_inh_factor, 2)*0.8,\n",
    "                'p_intra': 0.1,'delay': 0.2,'noise_type': 'poisson', 'step': False},\n",
    "        'connection': {'p_inter': 0.02,'weight_scale': k_factor*inter_k_factor}\n",
    "    }\n",
    "\n",
    "\n",
    "    delay_configs = {\n",
    "        f'MS_{names[i]}_coupling_step_input_uniform_0-6': {'type': 'uniform', 'params': {'low': 0.0, 'high': 6.0}},\n",
    "        f'MS_{names[i]}_coupling_step_input_uniform_0p75-5p25': {'type': 'uniform', 'params': {'low': 0.75, 'high': 5.25}},\n",
    "        f'MS_{names[i]}_coupling_step_input_uniform_1p5-4p5': {'type': 'uniform', 'params': {'low': 1.5, 'high': 4.5}},\n",
    "        f'MS_{names[i]}_coupling_step_input_uniform_2p25-p75': {'type': 'uniform', 'params': {'low': 2.25, 'high': 3.75}},\n",
    "    }\n",
    "        \n",
    "    # delay_configs = {\n",
    "    #     # === FORMAS EXTREMAS ===\n",
    "    #     # Forma de U - alta variabilidad (conexiones muy rápidas o muy lentas)\n",
    "    #     f'MS_{names[i]}_coupling_beta_U_shape': {'type': 'beta', 'params': {'alpha': 0.5, 'beta': 0.5, 'scale': 4.0}},\n",
    "        \n",
    "    #     # Uniforme - todos los retrasos igualmente probables\n",
    "    #     f'MS_{names[i]}_coupling_beta_uniform': {'type': 'beta', 'params': {'alpha': 1.0, 'beta': 1.0, 'scale': 4.0}},\n",
    "        \n",
    "    #     # === ASIMÉTRICAS (SESGADAS) ===\n",
    "    #     # Sesgada hacia retrasos cortos - la mayoría de conexiones son rápidas\n",
    "    #     f'MS_{names[i]}_coupling_beta_fast_bias': {'type': 'beta', 'params': {'alpha': 2.0, 'beta': 8.0, 'scale': 4.0}},\n",
    "    #     f'MS_{names[i]}_coupling_beta_very_fast': {'type': 'beta', 'params': {'alpha': 1.0, 'beta': 4.0, 'scale': 4.0}},\n",
    "        \n",
    "    #     # Sesgada hacia retrasos largos - dominan las conexiones lentas  \n",
    "    #     f'MS_{names[i]}_coupling_beta_slow_bias': {'type': 'beta', 'params': {'alpha': 8.0, 'beta': 2.0, 'scale': 4.0}},\n",
    "    #     f'MS_{names[i]}_coupling_beta_very_slow': {'type': 'beta', 'params': {'alpha': 4.0, 'beta': 1.0, 'scale': 4.0}},\n",
    "        \n",
    "    #     # === SIMÉTRICAS UNIMODALES ===\n",
    "    #     # Concentrada en el centro - retrasos intermedios más probables\n",
    "    #     f'MS_{names[i]}_coupling_beta_centered_narrow': {'type': 'beta', 'params': {'alpha': 4.0, 'beta': 4.0, 'scale': 4.0}},\n",
    "    #     f'MS_{names[i]}_coupling_beta_centered_sharp': {'type': 'beta', 'params': {'alpha': 8.0, 'beta': 8.0, 'scale': 4.0}},\n",
    "    #     f'MS_{names[i]}_coupling_beta_centered_very_sharp': {'type': 'beta', 'params': {'alpha': 12.0, 'beta': 12.0, 'scale': 4.0}},\n",
    "        \n",
    "    #     # === CASOS INTERMEDIOS ===\n",
    "    #     # Ligeramente sesgadas - sutiles preferencias direccionales\n",
    "    #     f'MS_{names[i]}_coupling_beta_mild_fast': {'type': 'beta', 'params': {'alpha': 3.0, 'beta': 5.0, 'scale': 4.0}},\n",
    "    #     f'MS_{names[i]}_coupling_beta_mild_slow': {'type': 'beta', 'params': {'alpha': 5.0, 'beta': 3.0, 'scale': 4.0}},\n",
    "        \n",
    "    #     # === DIFERENTES ESCALAS TEMPORALES ===\n",
    "    #     # Misma forma, diferentes rangos de retrasos\n",
    "    #     f'MS_{names[i]}_coupling_beta_short_range': {'type': 'beta', 'params': {'alpha': 4.0, 'beta': 4.0, 'scale': 2.0}},\n",
    "    #     f'MS_{names[i]}_coupling_beta_long_range': {'type': 'beta', 'params': {'alpha': 4.0, 'beta': 4.0, 'scale': 8.0}},\n",
    "    # }\n",
    "\n",
    "    # delay_configs = {\n",
    "    #     f'MS_{names[i]}_coupling_step_input_gauss_12-4': {'type': 'gaussian', 'params': {'mu': 12.0, 'sigma': 4.0}},\n",
    "    #     f'MS_{names[i]}_coupling_step_input_gauss_12-8': {'type': 'gaussian', 'params': {'mu': 12.0, 'sigma': 8.0}},\n",
    "    #     f'MS_{names[i]}_coupling_step_input_gauss_12-12': {'type': 'gaussian', 'params': {'mu': 12.0, 'sigma': 12.0}},\n",
    "    #     f'MS_{names[i]}_coupling_step_input_gauss_12-16': {'type': 'gaussian', 'params': {'mu': 12.0, 'sigma': 16.0}},\n",
    "    #     f'MS_{names[i]}_coupling_step_input_gauss_12-20': {'type': 'gaussian', 'params': {'mu': 12.0, 'sigma': 20.0}},\n",
    "    #     f'MS_{names[i]}_coupling_step_input_gauss_12-24': {'type': 'gaussian', 'params': {'mu': 12.0, 'sigma': 24.0}},\n",
    "    # }\n",
    "\n",
    "    # delay_configs = {\n",
    "    #     f'MS_{names[i]}_coupling_step_input_delta_0': {'type': 'constant', 'value': 0.0},\n",
    "    #     f'MS_{names[i]}_coupling_step_input_delta_4': {'type': 'constant', 'value': 4.0},\n",
    "    #     f'MS_{names[i]}_coupling_step_input_delta_8': {'type': 'constant', 'value': 8.0},\n",
    "    #     f'MS_{names[i]}_coupling_step_input_delta_12': {'type': 'constant', 'value': 12.0},\n",
    "    #     f'MS_{names[i]}_coupling_step_input_delta_16': {'type': 'constant', 'value': 16.0},\n",
    "    #     f'MS_{names[i]}_coupling_step_input_delta_20': {'type': 'constant', 'value': 20.0},\n",
    "    # }\n",
    "\n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    sweep_dir = create_sweep_folder(\n",
    "        base_dir=\"./results/experiments/two_populations/step_input\",\n",
    "        tag=TAG, timestamp=ts\n",
    "    )\n",
    "\n",
    "    config_dict = {\n",
    "        \"tag\": TAG, \"timestamp\": ts,\n",
    "        \"device_mode\": DEVICE_MODE,\n",
    "        \"dt_ms\": DT_MS, \"T_total_ms\": T_MS,\n",
    "        \"n_trials\": N_TRIALS, \"directionality\": DIRECTIONALITY,\n",
    "        \"base_params\": deepcopy(base_params),\n",
    "        \"delay_configs\": deepcopy(delay_configs),\n",
    "        \"seeds_variable\": [50+i for i in range(N_TRIALS)],\n",
    "        \"seed_fixed\": 42,\n",
    "        \"control_type\": \"strict\"\n",
    "    }\n",
    "    save_config_json(config_dict, os.path.join(sweep_dir, \"config.json\"))\n",
    "\n",
    "\n",
    "    # ==== CORRER SWEEP CON MULTIPROCESSING OPTIMIZADO ====\n",
    "    config_args = [\n",
    "        (name, config, sweep_dir, DT_MS, T_MS, base_params, N_TRIALS, DEVICE_MODE, DIRECTIONALITY)\n",
    "        for name, config in delay_configs.items()\n",
    "    ]\n",
    "\n",
    "    # En el proceso principal:\n",
    "    with multiprocessing.Pool(3) as pool:\n",
    "        completed_configs = pool.map_async(run_single_config, config_args).get(timeout=1800)\n",
    "\n",
    "    print(f\" --- COMPLETED: {completed_configs}\")\n",
    "    \n",
    "    # Reconstruir datos desde CSVs para plot de comparación\n",
    "    results_db_light = load_results_from_csvs(sweep_dir, completed_configs)\n",
    "\n",
    "    fig = plot_delay_distribution_comparison(results_db_light, save_path=os.path.join(sweep_dir, \"comparison.png\"))\n",
    "    \n",
    "    print(f\"Sweep folder: {sweep_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd66c028",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.two_populations.plots import load_trials_data_for_dashboard, plot_advanced_metrics_dashboard\n",
    "import os\n",
    "\n",
    "sweep_dir = './results/experiments/two_populations/step_input/MS_UNIFORM_1_SAME_NOISE_CI_STEP_MEDIUM_0p5_k3p5_dt0p1_T4000_30trials__20250919_183201'\n",
    "\n",
    "config_dirs = [d for d in os.listdir(sweep_dir) if d.startswith('config_')]\n",
    "completed_configs = [d.replace('config_', '') for d in config_dirs]\n",
    "\n",
    "results_db = load_results_from_csvs(sweep_dir, completed_configs, filter_outliers=True)\n",
    "trials_data = load_trials_data_for_dashboard(sweep_dir, completed_configs)\n",
    "\n",
    "fig = plot_advanced_metrics_dashboard(results_db, trials_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaebc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.two_populations.plots import extract_delay_parameters, extract_metrics_data, plot_correlation_analysis, print_correlation_summary\n",
    "\n",
    "delay_df = extract_delay_parameters(results_db)\n",
    "metrics_df = extract_metrics_data(results_db)\n",
    "fig, corr_matrix, p_matrix = plot_correlation_analysis(delay_df, metrics_df)\n",
    "print_correlation_summary(corr_matrix, p_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29260ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "print(f\"RAM disponible: {psutil.virtual_memory().available / 1e9:.1f} GB\")\n",
    "print(f\"RAM total: {psutil.virtual_memory().total / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b97d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import brian2\n",
    "print(f\"Brian2 version: {brian2.__version__}\")\n",
    "print(f\"Device: {brian2.get_device()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac5e7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ==== setup ====\n",
    "time = 3500\n",
    "dt_ms = 0.1\n",
    "n_timesteps = int(time/dt_ms)\n",
    "n_neurons = 800\n",
    "\n",
    "# “estado sano” como referencia\n",
    "lambda_exc = 5.1**2 / dt_ms\n",
    "lambda_inh = 2.0**2 / dt_ms\n",
    "\n",
    "def step_profile(n, start=0.25, end=0.75, base=0.0, elev=1.0):\n",
    "    prof = np.full(n, base, dtype=float); s,e = int(n*start), int(n*end)\n",
    "    prof[s:e] = elev; return prof\n",
    "\n",
    "# perfiles (μ = drive DC ; σ = factor de varianza)\n",
    "# mu_exc    = step_profile(n_timesteps, 0.25, 0.75, base=0.7*lambda_exc,  elev=1.15*lambda_exc)\n",
    "# sigma_exc = step_profile(n_timesteps, 0.25, 0.75, base=0.9,            elev=1.6)\n",
    "\n",
    "# mu_inh    = step_profile(n_timesteps, 0.25, 0.75, base=0.6*lambda_inh, elev=0.9*lambda_inh)\n",
    "# sigma_inh = step_profile(n_timesteps, 0.25, 0.75, base=0.9,            elev=1.2)\n",
    "\n",
    "mu_exc    = step_profile(n_timesteps, 0.25, 0.75, base=0.0,  elev=0.0)\n",
    "sigma_exc = step_profile(n_timesteps, 0.25, 0.75, base=1.0,   elev=2.0)\n",
    "\n",
    "mu_inh    = step_profile(n_timesteps, 0.25, 0.75, base=0.0, elev=0.0)\n",
    "sigma_inh = step_profile(n_timesteps, 0.25, 0.75, base=1.0,  elev=1.0)\n",
    "\n",
    "# ruido centrado (z ~ var≈1) y composición I = μ + σ·z\n",
    "# z_exc = (np.random.poisson(lambda_exc, (n_timesteps, n_neurons)) - 0*lambda_exc)#/np.sqrt(lambda_exc)\n",
    "# z_inh = (np.random.poisson(lambda_inh, (n_timesteps, n_neurons)) - 0*lambda_inh)#/np.sqrt(lambda_inh)\n",
    "z_exc = (np.random.randn(n_timesteps, n_neurons) - 0*lambda_exc)#/np.sqrt(lambda_exc)\n",
    "z_inh = (np.random.randn(n_timesteps, n_neurons) - 0*lambda_inh)#/np.sqrt(lambda_inh)\n",
    "I_exc = sigma_exc[:,None]*z_exc\n",
    "I_inh =  sigma_inh[:,None]*z_inh\n",
    "\n",
    "# ==== visualización ====\n",
    "t = np.arange(n_timesteps)*dt_ms\n",
    "neuron = 0\n",
    "win = int(500/dt_ms)         # 500 ms\n",
    "hop = max(1, win//4)\n",
    "\n",
    "def rolling_std(x, w, h):\n",
    "    out_t, out = [], []\n",
    "    for i in range(0, len(x)-w, h):\n",
    "        seg = x[i:i+w]\n",
    "        out_t.append((i+w/2)*dt_ms)\n",
    "        out.append(np.std(seg - seg.mean()))  # std centrada (solo variabilidad)\n",
    "    return np.array(out_t), np.array(out)\n",
    "\n",
    "fig, axes = plt.subplots(3,1, figsize=(12,8), sharex=False)\n",
    "\n",
    "# (1) perfiles μ(t) y σ(t)\n",
    "axes[0].plot(t, mu_exc,  label='μ_exc (DC)', lw=2)\n",
    "axes[0].plot(t, mu_inh,  label='μ_inh (DC)', lw=2, ls='--')\n",
    "axes[0].plot(t, sigma_exc*np.sqrt(lambda_exc), label='σ_exc·√λ_exc (escala var)', lw=1.5)\n",
    "axes[0].plot(t, sigma_inh*np.sqrt(lambda_inh), label='σ_inh·√λ_inh (escala var)', lw=1.5, ls='--')\n",
    "axes[0].set_title('Perfiles de modulación (media y variabilidad)')\n",
    "axes[0].set_ylabel('Amplitud'); axes[0].grid(True, alpha=0.3); axes[0].legend()\n",
    "\n",
    "# (2) neurona ejemplo: base vs modulado (excitatoria)\n",
    "axes[1].plot(t, z_exc[:,neuron],  lw=0.6, alpha=0.7, label='z_exc (base, μ=0, σ≈1)')\n",
    "axes[1].plot(t, I_exc[:,neuron], lw=0.8, alpha=0.9, label='I_exc = μ_exc + σ_exc·z')\n",
    "axes[1].set_title(f'Neurona excitatoria #{neuron}: señal base vs modulada')\n",
    "axes[1].set_ylabel('Entrada'); axes[1].grid(True, alpha=0.3); axes[1].legend()\n",
    "\n",
    "# (3) variabilidad temporal (std centrada en ventana deslizante)\n",
    "tb, std_base = rolling_std(z_exc[:,neuron], win, hop)\n",
    "tm, std_mod  = rolling_std(I_exc[:,neuron], win, hop)\n",
    "axes[2].plot(tb, std_base, lw=2, label='STD base (z centrado)')\n",
    "axes[2].plot(tm, std_mod,  lw=2, label='STD modulada (μ+σ·z, centrada)')\n",
    "axes[2].set_title('Evolución de la variabilidad (500 ms, solapamiento 75%)')\n",
    "axes[2].set_xlabel('Tiempo (ms)'); axes[2].set_ylabel('STD'); axes[2].grid(True, alpha=0.3); axes[2].legend()\n",
    "\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d9a0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.hist(np.random.poisson(2.0, (1000, 2))[:,0]+np.random.poisson(2.0, (1000, 2))[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66916b9d",
   "metadata": {},
   "source": [
    "== Estadísticas del Grupo A ===\n",
    "\n",
    "=== INPUT TALÁMICO ===\n",
    "Brian2 exc - Media por neurona: -0.0019\n",
    "Brian2 exc - Std por neurona: 4.9974\n",
    "\n",
    "=== INPUT SINÁPTICO ===\n",
    "Brian2 exc - Media: 0.8016\n",
    "Brian2 exc - Std: 0.5017\n",
    "Brian2 exc - Rango típico: [-4.84, 9.81]\n",
    "\n",
    "=== Estadísticas del Grupo B ===\n",
    "\n",
    "=== INPUT TALÁMICO ===\n",
    "Brian2 exc - Media por neurona: -0.0039\n",
    "Brian2 exc - Std por neurona: 5.0036\n",
    "\n",
    "=== INPUT SINÁPTICO ===\n",
    "Brian2 exc - Media: 0.8246\n",
    "Brian2 exc - Std: 0.5066\n",
    "Brian2 exc - Rango típico: [-4.62, 6.72]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef90223f",
   "metadata": {},
   "outputs": [],
   "source": [
    "peaks = []\n",
    "\n",
    "for time in np.arange(0,2600,50):\n",
    "\n",
    "    warmup = time\n",
    "\n",
    "    results_dict = {}\n",
    "\n",
    "    # Una línea para analizar\n",
    "    results_dict = {\n",
    "        'baseline': analyze_simulation_results(results['A']['spike_monitor'], results['B']['spike_monitor'], 1000, \"Baseline\", warmup=warmup)\n",
    "    }\n",
    "    \n",
    "    peaks.append(results_dict['baseline']['cross_correlation']['peak_value'])\n",
    "\n",
    "plt.plot(np.arange(0,2600,50), peaks, 'o-')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
