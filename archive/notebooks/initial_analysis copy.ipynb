{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177be8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2a3cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abrir archivo .txt con nombres de ROIs\n",
    "def load_roi_names(txt_file):\n",
    "    with open(txt_file, 'r') as f:\n",
    "        roi_names = [line.strip() for line in f.readlines()]\n",
    "    return roi_names\n",
    "\n",
    "# Abrir archivo .dat (diccionario Python)\n",
    "def load_rat_data(dat_file):\n",
    "    with open(dat_file, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "# Explorar estructura de datos\n",
    "def explore_data(roi_names, rat_data):\n",
    "    print(\"=== NOMBRES DE ROIs ===\")\n",
    "    print(f\"Número total de ROIs: {len(roi_names)}\")\n",
    "    print(\"Primeros 10 ROIs:\")\n",
    "    for i, roi in enumerate(roi_names[:10]):\n",
    "        print(f\"{i}: {roi}\")\n",
    "    \n",
    "    print(\"\\n=== ESTRUCTURA DEL DICCIONARIO ===\")\n",
    "    print(f\"Tipo de datos: {type(rat_data)}\")\n",
    "    print(f\"Claves principales: {list(rat_data.keys())}\")\n",
    "    \n",
    "    for key, value in rat_data.items():\n",
    "        print(f\"\\nClave '{key}':\")\n",
    "        print(f\"  Tipo: {type(value)}\")\n",
    "        if isinstance(value, np.ndarray):\n",
    "            print(f\"  Shape: {value.shape}\")\n",
    "            print(f\"  Dtype: {value.dtype}\")\n",
    "        elif isinstance(value, list):\n",
    "            print(f\"  Longitud: {len(value)}\")\n",
    "            if len(value) > 0:\n",
    "                print(f\"  Tipo elementos: {type(value[0])}\")\n",
    "        elif isinstance(value, dict):\n",
    "            print(f\"  Sub-claves: {list(value.keys())}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bf5130",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data/Toni_2025-08-06/'\n",
    "files = os.listdir(path)\n",
    "path+files[0], path+files[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cafae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_names = load_roi_names(path+files[0])\n",
    "roi_names, len(roi_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cdc23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rat_data = load_rat_data(path+files[1])\n",
    "explore_data(roi_names, rat_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6bc9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer pares de ROIs\n",
    "roi_pairs = list(rat_data.keys())\n",
    "\n",
    "# Análisis de pares\n",
    "pairs_df = pd.DataFrame(roi_pairs, columns=['roi_i', 'roi_j'])\n",
    "print(f\"Total conexiones: {len(pairs_df)}\")\n",
    "print(f\"Rango ROIs: {pairs_df.min().min()} - {pairs_df.max().max()}\")\n",
    "\n",
    "# Verificar estructura de listas\n",
    "sample_key = roi_pairs[1]\n",
    "sample_data = rat_data[sample_key]\n",
    "print(f\"\\nEjemplo conexión {sample_key}:\")\n",
    "print(f\"Tipo: {type(sample_data)}\")\n",
    "print(f\"Longitud: {len(sample_data)}\")\n",
    "print(f\"Primeros valores: {sample_data[:5]}\")\n",
    "\n",
    "lengths = []\n",
    "\n",
    "for i in range(len(pairs_df)):\n",
    "\n",
    "    key = (pairs_df.loc[i, 'roi_i'], pairs_df.loc[i, 'roi_j'])\n",
    "    \n",
    "    lengths.append(len(rat_data[key]))\n",
    "    \n",
    "plt.hist(lengths, bins=100, range = (0,1000)), max(lengths)\n",
    "plt.title(files[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded64724",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Cargar datos\n",
    "def load_data():\n",
    "    # ROIs\n",
    "    roi_names = pd.read_csv('./data/Toni_2025-08-06/atlas_cg_3d5_names.txt', \n",
    "                       sep=';', header=None, names=['roi_name'], engine='python')\n",
    "    \n",
    "    # Datos rata\n",
    "    with open('./data/Toni_2025-08-06/th-0.0_R01_b20_r_Fit_Histogram_Tau_all_fibers.dat', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    return roi_names, data\n",
    "\n",
    "# Explorar estructura de conectividad\n",
    "def explore_connectivity_structure(data, roi_names):\n",
    "    # Extraer pares de ROIs\n",
    "    roi_pairs = list(data.keys())\n",
    "    \n",
    "    # Análisis de pares\n",
    "    pairs_df = pd.DataFrame(roi_pairs, columns=['roi_i', 'roi_j'])\n",
    "    print(f\"Total conexiones: {len(pairs_df)}\")\n",
    "    print(f\"Rango ROIs: {pairs_df.min().min()} - {pairs_df.max().max()}\")\n",
    "    \n",
    "    # Verificar estructura de listas\n",
    "    sample_key = roi_pairs[1]\n",
    "    sample_data = data[sample_key]\n",
    "    print(f\"\\nEjemplo conexión {sample_key}:\")\n",
    "    print(f\"Tipo: {type(sample_data)}\")\n",
    "    print(f\"Longitud: {len(sample_data)}\")\n",
    "    print(f\"Primeros valores: {sample_data[:5]}\")\n",
    "    \n",
    "    return pairs_df\n",
    "\n",
    "# Convertir a matriz de conectividad\n",
    "def create_connectivity_matrix(data, roi_names, metric='mean'):\n",
    "    n_rois = len(roi_names)\n",
    "    \n",
    "    # Crear matriz vacía\n",
    "    connectivity_matrix = np.full((n_rois, n_rois), np.nan)\n",
    "    \n",
    "    # Llenar matriz\n",
    "    for (i, j), values in data.items():\n",
    "        if i < n_rois and j < n_rois:  # Verificar índices válidos\n",
    "            if metric == 'mean':\n",
    "                connectivity_matrix[i, j] = np.mean(values)\n",
    "            elif metric == 'std':\n",
    "                connectivity_matrix[i, j] = np.std(values)\n",
    "            elif metric == 'length':\n",
    "                connectivity_matrix[i, j] = len(values)\n",
    "    \n",
    "    # Convertir a DataFrame\n",
    "    matrix_df = pd.DataFrame(connectivity_matrix, \n",
    "                           index=roi_names['roi_name'], \n",
    "                           columns=roi_names['roi_name'])\n",
    "    \n",
    "    return matrix_df\n",
    "\n",
    "# Ejecutar análisis\n",
    "roi_names, data = load_data()\n",
    "pairs_df = explore_connectivity_structure(data, roi_names)\n",
    "\n",
    "# Crear matrices de conectividad\n",
    "conn_mean = create_connectivity_matrix(data, roi_names, 'mean')\n",
    "conn_std = create_connectivity_matrix(data, roi_names, 'std') \n",
    "conn_length = create_connectivity_matrix(data, roi_names, 'length')\n",
    "\n",
    "print(f\"\\nMatriz de conectividad (promedio):\")\n",
    "print(conn_mean.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a4acac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_connectivity_matrices(data, roi_names):\n",
    "    # Índices únicos\n",
    "    all_indices = sorted(set().union(*[{i, j} for i, j in data.keys()]))\n",
    "    n_rois = len(all_indices)\n",
    "    index_map = {idx: pos for pos, idx in enumerate(all_indices)}\n",
    "    \n",
    "    # ROI labels\n",
    "    roi_labels = [roi_names.iloc[idx]['roi_name'] if idx < len(roi_names) \n",
    "                  else f\"ROI_{idx}\" for idx in all_indices]\n",
    "    \n",
    "    # Matrices diferentes\n",
    "    matrices = {}\n",
    "    \n",
    "    for (i, j), measurements in data.items():\n",
    "        pos_i, pos_j = index_map[i], index_map[j]\n",
    "        \n",
    "        if not measurements:  # Conexión vacía\n",
    "            continue\n",
    "            \n",
    "        # Extraer valores\n",
    "        vals_0 = [m[0] for m in measurements]\n",
    "        vals_1 = [m[1] for m in measurements]\n",
    "        \n",
    "        # Llenar matrices (inicializar si no existe)\n",
    "        for name in ['val0_mean', 'val1_mean', 'n_measurements', 'val0_std', 'val1_std']:\n",
    "            if name not in matrices:\n",
    "                matrices[name] = np.full((n_rois, n_rois), np.nan)\n",
    "        \n",
    "        matrices['val0_mean'][pos_i, pos_j] = np.mean(vals_0)\n",
    "        matrices['val1_mean'][pos_i, pos_j] = np.mean(vals_1)\n",
    "        matrices['n_measurements'][pos_i, pos_j] = len(measurements)\n",
    "        matrices['val0_std'][pos_i, pos_j] = np.std(vals_0) if len(vals_0) > 1 else 0\n",
    "        matrices['val1_std'][pos_i, pos_j] = np.std(vals_1) if len(vals_1) > 1 else 0\n",
    "    \n",
    "    # Convertir a DataFrames\n",
    "    result = {}\n",
    "    for name, matrix in matrices.items():\n",
    "        result[name] = pd.DataFrame(matrix, index=roi_labels, columns=roi_labels)\n",
    "    \n",
    "    return result, all_indices\n",
    "\n",
    "# Crear matrices\n",
    "matrices, roi_indices = create_connectivity_matrices(data, roi_names)\n",
    "\n",
    "# Resumen\n",
    "for name, df in matrices.items():\n",
    "    print(f\"\\n{name.upper()}:\")\n",
    "    print(f\"Conexiones no-NaN: {(~df.isna()).sum().sum()}\")\n",
    "    print(f\"Rango: {df.min().min():.2e} - {df.max().max():.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a88d1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar matrices principales\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# VAL0 mean\n",
    "im1 = axes[0,0].imshow(matrices['val0_mean'].values, cmap='viridis', aspect='auto')\n",
    "axes[0,0].set_title('VAL0 Mean')\n",
    "plt.colorbar(im1, ax=axes[0,0])\n",
    "\n",
    "# VAL1 mean  \n",
    "im2 = axes[0,1].imshow(matrices['val1_mean'].values, cmap='viridis', aspect='auto')\n",
    "axes[0,1].set_title('VAL1 Mean')\n",
    "plt.colorbar(im2, ax=axes[0,1])\n",
    "\n",
    "# Número de mediciones\n",
    "im3 = axes[1,0].imshow(matrices['n_measurements'].values, cmap='plasma', aspect='auto')\n",
    "axes[1,0].set_title(f'N Measurements - {files[1]}')\n",
    "plt.colorbar(im3, ax=axes[1,0])\n",
    "\n",
    "# Correlación VAL0 vs VAL1\n",
    "val0_flat = matrices['val0_mean'].values.flatten()\n",
    "val1_flat = matrices['val1_mean'].values.flatten()\n",
    "mask = ~(np.isnan(val0_flat) | np.isnan(val1_flat))\n",
    "axes[1,1].scatter(val0_flat[mask], val1_flat[mask], alpha=0.5, s=1)\n",
    "axes[1,1].set_xlabel(f'VAL0 - {files[1]}')\n",
    "axes[1,1].set_ylabel(f'VAL1 - {files[1]}')\n",
    "axes[1,1].set_title(f'VAL0 vs VAL1 - {files[1]}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Estadísticas por ROI\n",
    "roi_stats = pd.DataFrame({\n",
    "    'roi_index': roi_indices,\n",
    "    'roi_name': [roi_names.iloc[i]['roi_name'] if i < len(roi_names) else f\"ROI_{i}\" \n",
    "                 for i in roi_indices],\n",
    "    'out_connections': (~matrices['val1_mean'].isna()).sum(axis=1),\n",
    "    'in_connections': (~matrices['val1_mean'].isna()).sum(axis=0),\n",
    "    'mean_val1_out': matrices['val1_mean'].mean(axis=1),\n",
    "    'mean_val1_in': matrices['val1_mean'].mean(axis=0)\n",
    "})\n",
    "\n",
    "print(roi_stats.nlargest(10, 'out_connections')[['roi_name', 'out_connections', 'in_connections']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbba295c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(roi_stats['in_connections'] > 0)/(len(roi_stats)*len(roi_stats)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee430ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_stats['out_connections'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1570f818",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(list(data.keys()))[np.array(list(data.keys()))[:,1] == 15,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e63d1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(list(data.keys()))[np.array(list(data.keys()))[:,0] == 35,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48df6193",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(np.array(list(data.keys()))[:,0]).shape , np.unique(np.array(list(data.keys()))[:,1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6824f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(data[(35,  36)]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89fc8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_i = np.unique(np.array(list(data.keys()))[:,0])\n",
    "unique_j = np.unique(np.array(list(data.keys()))[:,1])\n",
    "print(f\"Rango IDs: {unique_i.min()}-{unique_i.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dde383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar si hay patrón hemisférico\n",
    "left_ids = [id for id in unique_i if id < 80]  # ejemplo\n",
    "right_ids = [id for id in unique_i if id >= 80]\n",
    "\n",
    "# O verificar divisiones por rangos\n",
    "print(f\"IDs 3-81: {sum(1 for id in unique_i if 3 <= id <= 81)}\")\n",
    "print(f\"IDs 82-155: {sum(1 for id in unique_i if 82 <= id <= 155)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293896b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer todos los valores para análisis\n",
    "import numpy as np\n",
    "\n",
    "all_vals = []\n",
    "for measurements in data.values():\n",
    "    for measurement in measurements:\n",
    "        if len(measurement) == 3:\n",
    "            all_vals.append(measurement)\n",
    "\n",
    "vals_array = np.array(all_vals)\n",
    "vals_array = vals_array[~np.isnan(vals_array).any(axis=1)]\n",
    "\n",
    "# Estadísticas por columna\n",
    "for i in range(3):\n",
    "    print(f\"Val{i}: min={vals_array[:,i].min():.6f}, max={vals_array[:,i].max():.6f}, mean={vals_array[:,i].mean():.6f}\")\n",
    "\n",
    "# Correlaciones esperadas\n",
    "print(f\"Corr(Val3,Val4): {np.corrcoef(vals_array[:,0], vals_array[:,1])[0,1]:.3f}\")  # velocidad vs distancia\n",
    "print(f\"Corr(Val2,Val4): {np.corrcoef(vals_array[:,1], vals_array[:,2])[0,1]:.3f}\")  # tau vs distancia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a3318d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# Extraer datos de velocidad por conexión\n",
    "connection_strenght = defaultdict(list)\n",
    "unknown = defaultdict(list)\n",
    "connection_taus = defaultdict(list)\n",
    "\n",
    "for (i,j), measurements in data.items():\n",
    "    if len(measurements) > 0:\n",
    "        for measurement in measurements:\n",
    "            if len(measurement) == 3 and not np.isnan(measurement[0]):\n",
    "                connection_strenght[(i,j)].append(measurement[0])  # Val3\n",
    "                unknown[(i,j)].append(measurement[1])   # Val4\n",
    "                connection_taus[(i,j)].append(measurement[2])        # Val2\n",
    "\n",
    "# Crear DataFrame para análisis\n",
    "plot_data = []\n",
    "for conn, unk in unknown.items():\n",
    "    if len(unknown) >= 10:  # Solo conexiones con suficientes datos\n",
    "        plot_data.append({\n",
    "            'connection': f\"{conn[0]}-{conn[1]}\",\n",
    "            'unk': unk,\n",
    "            'mean_unk': np.mean(unk),\n",
    "            'std_unk': np.std(unk),\n",
    "            'cv_unk': np.std(unk)/np.mean(unk),\n",
    "            'n_measurements': len(unk),\n",
    "            'mean_strenght': np.mean(connection_strenght[conn]),\n",
    "            'mean_tau': np.mean(connection_taus[conn])\n",
    "        })\n",
    "\n",
    "df_connections = pd.DataFrame(plot_data)\n",
    "df_connections = df_connections.sort_values('n_measurements', ascending=False)\n",
    "\n",
    "# Crear plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Histograma general de velocidades\n",
    "all_velocities = [v for unk in unknown.values() for v in unk]\n",
    "axes[0,0].hist(all_velocities, bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[0,0].set_xlabel('unknown')\n",
    "axes[0,0].set_ylabel('Frecuencia')\n",
    "axes[0,0].set_title('Distribución General de unknown')\n",
    "axes[0,0].axvline(np.mean(all_velocities), color='red', linestyle='--', \n",
    "                  label=f'Media: {np.mean(all_velocities):.2f}')\n",
    "axes[0,0].legend()\n",
    "\n",
    "# 2. Coeficiente de variación por conexión\n",
    "axes[0,1].scatter(df_connections['mean_unk'], df_connections['cv_unk'], \n",
    "                  s=df_connections['n_measurements']*3, alpha=0.6)\n",
    "axes[0,1].set_xlabel('unknown')\n",
    "axes[0,1].set_ylabel('Coeficiente Variación')\n",
    "axes[0,1].set_title('Consistencia de unknown por Conexión\\n(tamaño = n_mediciones)')\n",
    "\n",
    "# 3. Velocidad vs Distancia por conexión\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(df_connections.head(20))))\n",
    "for color_idx, (idx, row) in enumerate(df_connections.head(20).iterrows()):\n",
    "    axes[1,0].scatter(row['mean_tau'], row['mean_unk'], \n",
    "                      c=[colors[color_idx]], s=row['n_measurements']*2, \n",
    "                      alpha=0.7)\n",
    "axes[1,0].set_xlabel('Distancia Media (mm)')\n",
    "axes[1,0].set_ylabel('Velocidad Media (m/s)')\n",
    "axes[1,0].set_title('Velocidad vs Distancia (Top 20 conexiones)')\n",
    "\n",
    "# 4. Boxplot de velocidades para top conexiones\n",
    "top_connections = df_connections.head(12)\n",
    "vel_data = [unknown[(int(conn.split('-')[0]), int(conn.split('-')[1]))] \n",
    "            for conn in top_connections['connection']]\n",
    "axes[1,1].boxplot(vel_data, labels=[f\"{conn[:8]}...\" for conn in top_connections['connection']])\n",
    "axes[1,1].set_ylabel('Velocidad (m/s)')\n",
    "axes[1,1].set_title('Distribución unknown por Conexión')\n",
    "axes[1,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Estadísticas de consistencia\n",
    "print(\"=== ANÁLISIS DE CONSISTENCIA DE unknown ===\")\n",
    "print(f\"Conexiones con ≥10 mediciones: {len(df_connections)}\")\n",
    "print(f\"CV promedio: {df_connections['cv_unk'].mean():.3f}\")\n",
    "print(f\"Conexiones muy variables (CV>0.5): {sum(df_connections['cv_unk'] > 0.5)}\")\n",
    "\n",
    "print(\"\\n=== TOP 10 CONEXIONES MÁS CONSISTENTES ===\")\n",
    "consistent = df_connections.nsmallest(10, 'cv_unk')[['connection', 'mean_unk', 'cv_unk', 'n_measurements']]\n",
    "print(consistent.to_string(index=False))\n",
    "\n",
    "print(\"\\n=== TOP 10 CONEXIONES MÁS VARIABLES ===\")\n",
    "variable = df_connections.nlargest(10, 'cv_unk')[['connection', 'mean_unk', 'cv_unk', 'n_measurements']]\n",
    "print(variable.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1c8d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# Extraer datos de velocidad por conexión\n",
    "connection_velocities = defaultdict(list)\n",
    "connection_distances = defaultdict(list)\n",
    "connection_taus = defaultdict(list)\n",
    "\n",
    "for (i,j), measurements in data.items():\n",
    "    if len(measurements) > 0:\n",
    "        for measurement in measurements:\n",
    "            if len(measurement) == 3 and not np.isnan(measurement[0]):\n",
    "                connection_velocities[(i,j)].append(measurement[0])  # Val3\n",
    "                connection_distances[(i,j)].append(measurement[1])   # Val4\n",
    "                connection_taus[(i,j)].append(measurement[2])        # Val2\n",
    "\n",
    "# Crear DataFrame para análisis\n",
    "plot_data = []\n",
    "for conn, velocities in connection_velocities.items():\n",
    "    if len(velocities) >= 10:  # Solo conexiones con suficientes datos\n",
    "        plot_data.append({\n",
    "            'connection': f\"{conn[0]}-{conn[1]}\",\n",
    "            'velocities': velocities,\n",
    "            'mean_vel': np.mean(velocities),\n",
    "            'std_vel': np.std(velocities),\n",
    "            'cv_vel': np.std(velocities)/np.mean(velocities),\n",
    "            'n_measurements': len(velocities),\n",
    "            'mean_dist': np.mean(connection_distances[conn]),\n",
    "            'mean_tau': np.mean(connection_taus[conn])\n",
    "        })\n",
    "\n",
    "df_connections = pd.DataFrame(plot_data)\n",
    "df_connections = df_connections.sort_values('n_measurements', ascending=False)\n",
    "\n",
    "# Crear plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Histograma general de velocidades\n",
    "all_velocities = [v for vels in connection_velocities.values() for v in vels]\n",
    "axes[0,0].hist(all_velocities, bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[0,0].set_xlabel('Velocidad (m/s)')\n",
    "axes[0,0].set_ylabel('Frecuencia')\n",
    "axes[0,0].set_title('Distribución General de Val_0')\n",
    "axes[0,0].axvline(np.mean(all_velocities), color='red', linestyle='--', \n",
    "                  label=f'Media: {np.mean(all_velocities):.2f}')\n",
    "axes[0,0].legend()\n",
    "\n",
    "# 2. Coeficiente de variación por conexión\n",
    "axes[0,1].scatter(df_connections['mean_vel'], df_connections['cv_vel'], \n",
    "                  s=df_connections['n_measurements']*3, alpha=0.6)\n",
    "axes[0,1].set_xlabel('Velocidad Media (m/s)')\n",
    "axes[0,1].set_ylabel('Coeficiente Variación')\n",
    "axes[0,1].set_title('Consistencia de Velocidad por Conexión\\n(tamaño = n_mediciones)')\n",
    "\n",
    "# 3. Velocidad vs Distancia por conexión\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(df_connections.head(20))))\n",
    "for color_idx, (idx, row) in enumerate(df_connections.head(20).iterrows()):\n",
    "    axes[1,0].scatter(row['mean_dist'], row['mean_vel'], \n",
    "                      c=[colors[color_idx]], s=row['n_measurements']*2, \n",
    "                      alpha=0.7)\n",
    "axes[1,0].set_xlabel('Distancia Media (mm)')\n",
    "axes[1,0].set_ylabel('Velocidad Media (m/s)')\n",
    "axes[1,0].set_title('Velocidad vs Distancia (Top 20 conexiones)')\n",
    "\n",
    "# 4. Boxplot de velocidades para top conexiones\n",
    "top_connections = df_connections.head(12)\n",
    "vel_data = [connection_velocities[(int(conn.split('-')[0]), int(conn.split('-')[1]))] \n",
    "            for conn in top_connections['connection']]\n",
    "axes[1,1].boxplot(vel_data, labels=[f\"{conn[:8]}...\" for conn in top_connections['connection']])\n",
    "axes[1,1].set_ylabel('Velocidad (m/s)')\n",
    "axes[1,1].set_title('Distribución Velocidades por Conexión')\n",
    "axes[1,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Estadísticas de consistencia\n",
    "print(\"=== ANÁLISIS DE CONSISTENCIA DE VELOCIDADES ===\")\n",
    "print(f\"Conexiones con ≥10 mediciones: {len(df_connections)}\")\n",
    "print(f\"CV promedio: {df_connections['cv_vel'].mean():.3f}\")\n",
    "print(f\"Conexiones muy variables (CV>0.5): {sum(df_connections['cv_vel'] > 0.5)}\")\n",
    "\n",
    "print(\"\\n=== TOP 10 CONEXIONES MÁS CONSISTENTES ===\")\n",
    "consistent = df_connections.nsmallest(10, 'cv_vel')[['connection', 'mean_vel', 'cv_vel', 'n_measurements']]\n",
    "print(consistent.to_string(index=False))\n",
    "\n",
    "print(\"\\n=== TOP 10 CONEXIONES MÁS VARIABLES ===\")\n",
    "variable = df_connections.nlargest(10, 'cv_vel')[['connection', 'mean_vel', 'cv_vel', 'n_measurements']]\n",
    "print(variable.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41c72be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Extraer datos para correlaciones\n",
    "velocities = []\n",
    "distances = []\n",
    "taus = []\n",
    "qualities = []\n",
    "\n",
    "for measurements in data.values():\n",
    "    for measurement in measurements:\n",
    "        if len(measurement) == 3:\n",
    "            v, d, t = measurement[0], measurement[1], measurement[2]\n",
    "            if not any(np.isnan([v, d, t])):\n",
    "                velocities.append(v)\n",
    "                distances.append(d)\n",
    "                taus.append(t)\n",
    "\n",
    "velocities = np.array(velocities)\n",
    "distances = np.array(distances)\n",
    "taus = np.array(taus)\n",
    "qualities = np.array(qualities)\n",
    "\n",
    "# Crear figura con subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Velocidad vs Distancia\n",
    "axes[0,0].scatter(distances, velocities, alpha=0.3, s=1)\n",
    "# Ajuste lineal\n",
    "slope_vd, intercept_vd, r_vd, p_vd, _ = stats.linregress(distances, velocities)\n",
    "line_vd = slope_vd * distances + intercept_vd\n",
    "axes[0,0].plot(distances, line_vd, 'r-', linewidth=2)\n",
    "axes[0,0].set_xlabel('Val_1 (mm)')\n",
    "axes[0,0].set_ylabel('Val_0 (m/s)')\n",
    "axes[0,0].set_title(f'Val_0 vs Val_1\\nr = {r_vd:.3f}, p < 0.001')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Velocidad vs Tau\n",
    "axes[0,1].scatter(taus, velocities, alpha=0.3, s=1)\n",
    "slope_vt, intercept_vt, r_vt, p_vt, _ = stats.linregress(taus, velocities)\n",
    "line_vt = slope_vt * taus + intercept_vt\n",
    "axes[0,1].plot(taus, line_vt, 'r-', linewidth=2)\n",
    "axes[0,1].set_xlabel('Val_2 (ms)')\n",
    "axes[0,1].set_ylabel('Val_0 (m/s)')\n",
    "axes[0,1].set_title(f'Val_0 vs Val_2\\nr = {r_vt:.3f}, p < 0.001')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Distribución de velocidades por rangos de calidad\n",
    "quality_bins = [0, 0.3, 0.5, 0.7, 1.0]\n",
    "quality_labels = ['<0.3', '0.3-0.5', '0.5-0.7', '0.7-1.0']\n",
    "vel_by_quality = []\n",
    "\n",
    "for i in range(len(quality_bins)-1):\n",
    "    mask = (qualities >= quality_bins[i]) & (qualities < quality_bins[i+1])\n",
    "    vel_by_quality.append(velocities[mask])\n",
    "\n",
    "axes[1,0].boxplot(vel_by_quality, labels=quality_labels)\n",
    "axes[1,0].set_xlabel('Calidad del Ajuste')\n",
    "axes[1,0].set_ylabel('Val_0 (m/s)')\n",
    "axes[1,0].set_title('Val_0 por Calidad del Modelo')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Tau vs Distancia (verificación física)\n",
    "axes[1,1].scatter(distances, taus, alpha=0.3, s=1)\n",
    "slope_td, intercept_td, r_td, p_td, _ = stats.linregress(distances, taus)\n",
    "line_td = slope_td * distances + intercept_td\n",
    "axes[1,1].plot(distances, line_td, 'r-', linewidth=2)\n",
    "axes[1,1].set_xlabel('Val_1 (mm)')\n",
    "axes[1,1].set_ylabel('Val_2 (ms)')\n",
    "axes[1,1].set_title(f'Val_1 vs Val_1\\nr = {r_td:.3f}, p < 0.001')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Estadísticas detalladas\n",
    "print(\"=== ANÁLISIS DE CORRELACIONES ===\")\n",
    "print(f\"Velocidad vs Distancia: r = {r_vd:.4f}, pendiente = {slope_vd:.4f} m/s/mm\")\n",
    "print(f\"Velocidad vs Tau: r = {r_vt:.4f}, pendiente = {slope_vt:.4f} m/s/ms\")\n",
    "print(f\"Tau vs Distancia: r = {r_td:.4f}, pendiente = {slope_td:.4f} ms/mm\")\n",
    "\n",
    "print(f\"\\n=== RANGOS DE VELOCIDADES ===\")\n",
    "print(f\"Velocidades < 2 m/s: {sum(velocities < 2)/len(velocities)*100:.1f}%\")\n",
    "print(f\"Velocidades 2-4 m/s: {sum((velocities >= 2) & (velocities <= 4))/len(velocities)*100:.1f}%\")\n",
    "print(f\"Velocidades > 4 m/s: {sum(velocities > 4)/len(velocities)*100:.1f}%\")\n",
    "print(f\"Velocidades > 6 m/s: {sum(velocities > 6)/len(velocities)*100:.1f}%\")\n",
    "\n",
    "# Velocidad promedio por distancia\n",
    "print(f\"\\n=== VELOCIDAD MEDIA POR RANGO DE DISTANCIA ===\")\n",
    "dist_ranges = [(0, 0.05), (0.05, 0.1), (0.1, 0.2), (0.2, 0.5), (0.5, 1.0)]\n",
    "for d_min, d_max in dist_ranges:\n",
    "    mask = (distances >= d_min) & (distances < d_max)\n",
    "    if sum(mask) > 0:\n",
    "        mean_vel = velocities[mask].mean()\n",
    "        print(f\"{d_min:.2f}-{d_max:.2f} mm: {mean_vel:.2f} ± {velocities[mask].std():.2f} m/s (n={sum(mask)})\")\n",
    "\n",
    "# Correlaciones por calidad\n",
    "# print(f\"\\n=== CORRELACIONES POR CALIDAD ===\")\n",
    "# high_quality = qualities > 0.5\n",
    "# print(f\"Alta calidad (>0.5): Vel-Dist r = {pearsonr(distances[high_quality], velocities[high_quality])[0]:.3f}\")\n",
    "# print(f\"Baja calidad (≤0.5): Vel-Dist r = {pearsonr(distances[~high_quality], velocities[~high_quality])[0]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f648b3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar si hay patrón hemisférico\n",
    "left_ids = [id for id in unique_i if id < 80]  # ejemplo\n",
    "right_ids = [id for id in unique_i if id >= 80]\n",
    "\n",
    "# O verificar divisiones por rangos\n",
    "print(f\"IDs 3-81: {sum(1 for id in unique_i if 3 <= id <= 81)}\")\n",
    "print(f\"IDs 82-155: {sum(1 for id in unique_i if 82 <= id <= 155)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f2b68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con tus datos 'data' y unique_i\n",
    "unique_i = np.unique(np.array(list(data.keys()))[:,0])\n",
    "\n",
    "# Análisis hemisférico\n",
    "left_ids = [id for id in unique_i if id < 80]\n",
    "right_ids = [id for id in unique_i if id >= 80]\n",
    "\n",
    "print(f\"IDs < 80: {len(left_ids)}\")\n",
    "print(f\"IDs >= 80: {len(right_ids)}\")\n",
    "\n",
    "# Rangos específicos  \n",
    "ids_3_81 = sum(1 for id in unique_i if 3 <= id <= 81)\n",
    "ids_82_155 = sum(1 for id in unique_i if 82 <= id <= 155)\n",
    "\n",
    "print(f\"IDs 3-81: {ids_3_81}\")\n",
    "print(f\"IDs 82-155: {ids_82_155}\")\n",
    "\n",
    "# Buscar patrón de offset\n",
    "for offset in [79, 78, 77]:\n",
    "    pairs = [(id, id+offset) for id in unique_i if id+offset in unique_i]\n",
    "    print(f\"Parejas con offset +{offset}: {len(pairs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87637c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def analyze_data_structure(data):\n",
    "    # Estadísticas de longitudes\n",
    "    lengths = [len(v) for v in data.values()]\n",
    "    \n",
    "    print(\"DISTRIBUCIÓN DE MEDICIONES POR CONEXIÓN:\")\n",
    "    print(f\"Min: {min(lengths)}, Max: {max(lengths)}, Media: {np.mean(lengths):.1f}\")\n",
    "    print(f\"Conexiones vacías: {sum(1 for l in lengths if l == 0)}\")\n",
    "    print(f\"Conexiones con datos: {sum(1 for l in lengths if l > 0)}\")\n",
    "    \n",
    "    # Histograma de longitudes\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(lengths, bins=50, alpha=0.7)\n",
    "    plt.xlabel('Número de mediciones')\n",
    "    plt.ylabel('Frecuencia')\n",
    "    plt.title('Distribución de mediciones por conexión')\n",
    "    \n",
    "    # Análisis de los 6 valores\n",
    "    non_empty = [v for v in data.values() if len(v) > 0]\n",
    "    sample_measurements = non_empty[0][:5]  # Primeras 5 mediciones\n",
    "    \n",
    "    print(f\"\\nANÁLISIS DE LOS 6 VALORES:\")\n",
    "    for i, measurement in enumerate(sample_measurements):\n",
    "        print(f\"Medición {i}: {[f'{val:.6f}' for val in measurement]}\")\n",
    "    \n",
    "    # Estadísticas por cada columna de los 6 valores\n",
    "    all_measurements = []\n",
    "    for measurements in non_empty[:100]:  # Muestra de 100 conexiones\n",
    "        all_measurements.extend(measurements)\n",
    "    \n",
    "    measurements_array = np.array(all_measurements)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.boxplot([measurements_array[:, i] for i in range(3)], \n",
    "                labels=[f'Valor_{i}' for i in range(3)])\n",
    "    plt.yscale('log')\n",
    "    plt.title('Distribución de los 6 valores')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nESTADÍSTICAS DE LOS 6 VALORES (muestra):\")\n",
    "    for i in range(3):\n",
    "        vals = measurements_array[:, i]\n",
    "        print(f\"Valor {i}: min={vals.min():.2e}, max={vals.max():.2e}, media={vals.mean():.2e}\")\n",
    "\n",
    "analyze_data_structure(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2311ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar ROIs más conectados y con mayor variabilidad\n",
    "def find_rich_rois(matrices, roi_indices, roi_names):\n",
    "    stats = []\n",
    "    \n",
    "    for i, roi_idx in enumerate(roi_indices):\n",
    "        roi_name = roi_names.iloc[roi_idx]['roi_name'] if roi_idx < len(roi_names) else f\"ROI_{roi_idx}\"\n",
    "        \n",
    "        # Conexiones salientes\n",
    "        out_connections = (~matrices['val1_mean'].iloc[i].isna()).sum()\n",
    "        out_variability = matrices['val1_std'].iloc[i].mean()\n",
    "        \n",
    "        # Conexiones entrantes  \n",
    "        in_connections = (~matrices['val1_mean'].iloc[:, i].isna()).sum()\n",
    "        in_variability = matrices['val1_std'].iloc[:, i].mean()\n",
    "        \n",
    "        # Total mediciones\n",
    "        total_measurements = matrices['n_measurements'].iloc[i].sum()\n",
    "        \n",
    "        stats.append({\n",
    "            'roi_idx': roi_idx,\n",
    "            'roi_name': roi_name,\n",
    "            'out_conn': out_connections,\n",
    "            'in_conn': in_connections,\n",
    "            'total_conn': out_connections + in_connections,\n",
    "            'out_var': out_variability,\n",
    "            'total_meas': total_measurements,\n",
    "            'richness': total_measurements * (out_connections + in_connections)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(stats).sort_values('richness', ascending=False)\n",
    "\n",
    "# Analizar ROIs más ricos\n",
    "rich_rois = find_rich_rois(matrices, roi_indices, roi_names)\n",
    "print(\"TOP 10 ROIs MÁS RICOS:\")\n",
    "print(rich_rois.head(10)[['roi_name', 'total_conn', 'total_meas', 'richness']])\n",
    "\n",
    "# Seleccionar 2 ROIs para análisis detallado\n",
    "top_rois = rich_rois.head(2)\n",
    "print(f\"\\nSELECCIONADOS: {top_rois['roi_name'].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f954427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar ROIs corticales\n",
    "cortical_keywords = ['cortex', 'area', 'Primary', 'Secondary', 'Cingulate', 'motor', 'visual', 'somatosensory']\n",
    "cortical_rois = rich_rois[rich_rois['roi_name'].str.contains('|'.join(cortical_keywords), case=False)]\n",
    "\n",
    "print(\"TOP ROIs CORTICALES:\")\n",
    "print(cortical_rois.head(8)[['roi_name', 'total_conn', 'total_meas']])\n",
    "\n",
    "# Analizar conexión específica entre dos ROIs corticales\n",
    "roi1_name = \"Primary somatosensory area\"  \n",
    "roi2_name = \"Primary motor area\"  # o el segundo más rico cortical\n",
    "\n",
    "# Encontrar índices\n",
    "roi1_idx = roi_indices[roi_names[roi_names['roi_name'] == roi1_name].index[0]]\n",
    "roi2_idx = roi_indices[roi_names[roi_names['roi_name'] == roi2_name].index[0]]\n",
    "\n",
    "print(f\"\\nConexión {roi1_name} ↔ {roi2_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0558a32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar conexiones cortico-corticales con mejor señal/ruido\n",
    "cortical_rois = {\n",
    "    8: 'Primary somatosensory area',\n",
    "    54: 'Primary motor area', \n",
    "    10: 'Secondary visual area',\n",
    "    3: 'Secondary auditory area',\n",
    "    40: 'Postrhinal cortex',\n",
    "    28: 'Cingulate area 2',\n",
    "    66: 'Prelimbic area',\n",
    "    67: 'Infralimbic area'\n",
    "}\n",
    "\n",
    "# Buscar conexiones cortico-corticales ricas\n",
    "def find_cortical_connections(data, cortical_dict):\n",
    "    cortical_connections = []\n",
    "    \n",
    "    for (i, j), measurements in data.items():\n",
    "        if i in cortical_dict and j in cortical_dict and len(measurements) > 50:\n",
    "            cortical_connections.append({\n",
    "                'connection': f\"{cortical_dict[i]} → {cortical_dict[j]}\",\n",
    "                'roi_pair': (i, j),\n",
    "                'n_measurements': len(measurements),\n",
    "                'val0_mean': np.mean([m[0] for m in measurements]),\n",
    "                'val1_mean': np.mean([m[1] for m in measurements])\n",
    "            })\n",
    "    \n",
    "    return sorted(cortical_connections, key=lambda x: x['n_measurements'], reverse=True)\n",
    "\n",
    "# Analizar conexiones cortico-corticales\n",
    "cortical_connections = find_cortical_connections(data, cortical_rois)\n",
    "\n",
    "print(\"TOP CONEXIONES CORTICO-CORTICALES:\")\n",
    "for conn in cortical_connections[:8]:\n",
    "    print(f\"{conn['connection']}: {conn['n_measurements']} mediciones\")\n",
    "\n",
    "# Seleccionar conexión específica para análisis detallado\n",
    "target_connection = cortical_connections[0] if cortical_connections else None\n",
    "print(f\"\\nCONEXIÓN SELECCIONADA: {target_connection['connection']}\")\n",
    "print(f\"Mediciones: {target_connection['n_measurements']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ce7509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analizar conexión Secondary visual area → Cingulate area 2\n",
    "roi_pair = (10, 28)  # Secondary visual area → Cingulate area 2\n",
    "measurements = data[roi_pair]\n",
    "\n",
    "print(f\"Análisis: Secondary visual area → Cingulate area 2\")\n",
    "print(f\"Total mediciones: {len(measurements)}\")\n",
    "\n",
    "# Extraer los 6 valores\n",
    "values_array = np.array(measurements)\n",
    "print(f\"Shape: {values_array.shape}\")\n",
    "\n",
    "# Estadísticas por valor\n",
    "for i in range(3):\n",
    "    vals = values_array[:, i]\n",
    "    print(f\"Valor {i}: min={vals.min():.6f}, max={vals.max():.6f}, \"\n",
    "          f\"mean={vals.mean():.6f}, std={vals.std():.6f}\")\n",
    "\n",
    "# Visualizar distribuciones\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 5))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(3):\n",
    "    axes[i].hist(values_array[:, i], bins=50, alpha=0.7)\n",
    "    axes[i].set_title(f'Valor {i}')\n",
    "    axes[i].set_xlabel('Valor')\n",
    "    axes[i].set_ylabel('Frecuencia')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlaciones entre valores\n",
    "import pandas as pd\n",
    "df_values = pd.DataFrame(values_array, columns=[f'Val_{i}' for i in range(3)])\n",
    "corr_matrix = df_values.corr()\n",
    "print(\"\\nMatriz de correlación:\")\n",
    "print(corr_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826f7c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analizar distribución de retrasos (Val_2)\n",
    "tau_values = values_array[:, 2]\n",
    "\n",
    "print(f\"ANÁLISIS DE RETRASOS TEMPORALES (TAU):\")\n",
    "print(f\"Media: {tau_values.mean():.3f}\")\n",
    "print(f\"Mediana: {np.median(tau_values):.3f}\")\n",
    "print(f\"Rango: {tau_values.min():.3f} - {tau_values.max():.3f}\")\n",
    "\n",
    "# Percentiles de interés\n",
    "percentiles = [10, 25, 50, 75, 90, 95, 99]\n",
    "for p in percentiles:\n",
    "    print(f\"P{p}: {np.percentile(tau_values, p):.3f}\")\n",
    "\n",
    "# Relación conectividad-retrasos\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(values_array[:, 0], tau_values, alpha=0.5, s=1)\n",
    "plt.xlabel('Conectividad (Val_0)')\n",
    "plt.ylabel('Tau (retrasos)')\n",
    "plt.title('Conectividad vs Retrasos')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(tau_values, bins=50, alpha=0.7)\n",
    "plt.xlabel('Tau (retrasos)')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.title('Distribución de retrasos')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11fc3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analizar distribuciones de retrasos en múltiples conexiones ricas\n",
    "target_connections = [\n",
    "    ((10, 28), \"Secondary visual area → Cingulate area 2\"),\n",
    "    ((28, 67), \"Cingulate area 2 → Infralimbic area\"), \n",
    "    ((28, 54), \"Cingulate area 2 → Primary motor area\"),\n",
    "    ((25, 30), \"Hippocampus → Septal region\"),\n",
    "    ((48, 25), \"Caudate putamen → Hippocampus\"),\n",
    "    ((8, 54), \"Primary somatosensory → Primary motor\"),\n",
    "]\n",
    "\n",
    "# Extraer retrasos por conexión\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "stats_summary = []\n",
    "\n",
    "for i, (roi_pair, name) in enumerate(target_connections):\n",
    "    if roi_pair in data and len(data[roi_pair]) > 50:\n",
    "        measurements = data[roi_pair]\n",
    "        tau_values = np.array([m[2] for m in measurements])\n",
    "        \n",
    "        # Estadísticas\n",
    "        stats = {\n",
    "            'connection': name,\n",
    "            'n_meas': len(measurements),\n",
    "            'tau_mean': tau_values.mean(),\n",
    "            'tau_median': np.median(tau_values),\n",
    "            'tau_std': tau_values.std(),\n",
    "            'tau_p95': np.percentile(tau_values, 95)\n",
    "        }\n",
    "        stats_summary.append(stats)\n",
    "        \n",
    "        # Plot\n",
    "        axes[i].hist(tau_values, bins=40, alpha=0.7, density=True)\n",
    "        axes[i].set_title(f\"{name}\\n(n={len(measurements)})\")\n",
    "        axes[i].set_xlabel('Tau (ms)')\n",
    "        axes[i].axvline(tau_values.mean(), color='red', linestyle='--', alpha=0.8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Resumen estadístico\n",
    "df_stats = pd.DataFrame(stats_summary)\n",
    "print(\"Estadísticas de retrasos por conexión:\")\n",
    "print(df_stats[['connection', 'n_meas', 'tau_mean', 'tau_median', 'tau_std']].round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89785b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar con conexiones subcorticales\n",
    "subcortical_pairs = [\n",
    "    ((25, 30), \"Hippocampus → Septal region\"),\n",
    "    ((48, 25), \"Caudate putamen → Hippocampus\"),\n",
    "    ((5, 48), \"Substantia nigra → Caudate putamen\")\n",
    "]\n",
    "\n",
    "print(\"Comparación cortico-cortical vs subcortical:\")\n",
    "# Código para analizar si subcorticales tienen diferentes rangos de tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39753ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROIs corticales ricos según criterios combinados\n",
    "top_cortical_rois = {\n",
    "    8: 'Primary somatosensory area',\n",
    "    10: 'Secondary visual area', \n",
    "    28: 'Cingulate area 2',\n",
    "    54: 'Primary motor area',\n",
    "    3: 'Secondary auditory area',\n",
    "    40: 'Postrhinal cortex',\n",
    "    66: 'Prelimbic area',\n",
    "    67: 'Infralimbic area'\n",
    "}\n",
    "\n",
    "# Encontrar conexiones cortico-corticales ricas\n",
    "def find_rich_cortical_connections(data, roi_dict, min_measurements=100):\n",
    "    connections = []\n",
    "    for (i, j), measurements in data.items():\n",
    "        if (i in roi_dict and j in roi_dict and \n",
    "            len(measurements) >= min_measurements):\n",
    "            tau_values = [m[2] for m in measurements]\n",
    "            connections.append({\n",
    "                'pair': (i, j),\n",
    "                'name': f\"{roi_dict[i]} → {roi_dict[j]}\",\n",
    "                'n_meas': len(measurements),\n",
    "                'tau_mean': np.mean(tau_values),\n",
    "                'tau_std': np.std(tau_values),\n",
    "                'tau_values': tau_values\n",
    "            })\n",
    "    return sorted(connections, key=lambda x: x['n_meas'], reverse=True)\n",
    "\n",
    "rich_connections = find_rich_cortical_connections(data, top_cortical_rois)\n",
    "\n",
    "# Visualizar top 6 conexiones\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, conn in enumerate(rich_connections[:6]):\n",
    "    axes[i].hist(conn['tau_values'], bins=30, alpha=0.7, density=True)\n",
    "    axes[i].set_title(f\"{conn['name']}\\nn={conn['n_meas']}\")\n",
    "    axes[i].set_xlabel('Tau (ms)')\n",
    "    axes[i].axvline(conn['tau_mean'], color='red', linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Resumen estadístico\n",
    "print(\"Top conexiones cortico-corticales:\")\n",
    "for conn in rich_connections[:8]:\n",
    "    print(f\"{conn['name']}: {conn['n_meas']} meas, τ={conn['tau_mean']:.2f}±{conn['tau_std']:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2240db52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expandir análisis a más conexiones (reducir umbral)\n",
    "rich_connections = find_rich_cortical_connections(data, top_cortical_rois, min_measurements=50)\n",
    "\n",
    "# Visualizar top 9 conexiones\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, conn in enumerate(rich_connections[:9]):\n",
    "    axes[i].hist(conn['tau_values'], bins=25, alpha=0.7, density=True, color=f'C{i}')\n",
    "    axes[i].set_title(f\"{conn['name'][:25]}...\\nn={conn['n_meas']}\")\n",
    "    axes[i].set_xlabel('Tau (ms)')\n",
    "    axes[i].axvline(conn['tau_mean'], color='red', linestyle='--', alpha=0.8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Tabla completa\n",
    "print(\"Todas las conexiones cortico-corticales ricas:\")\n",
    "for i, conn in enumerate(rich_connections[:12]):\n",
    "    print(f\"{i+1:2d}. {conn['name'][:50]:<50} {conn['n_meas']:4d} meas, τ={conn['tau_mean']:.2f}±{conn['tau_std']:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df30a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROIs subcorticales/límbicos ricos\n",
    "subcortical_rois = {\n",
    "    48: 'Caudate putamen',\n",
    "    25: 'Hippocampus', \n",
    "    24: 'Globus pallidus external',\n",
    "    30: 'Septal region',\n",
    "    5: 'Substantia nigra',\n",
    "    20: 'RT',\n",
    "    26: 'Subiculum',\n",
    "    27: 'Nucleus accumbens'\n",
    "}\n",
    "\n",
    "# Conexiones subcorticales ricas\n",
    "subcortical_connections = find_rich_cortical_connections(data, subcortical_rois, min_measurements=50)\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, conn in enumerate(subcortical_connections[:9]):\n",
    "    axes[i].hist(conn['tau_values'], bins=25, alpha=0.7, density=True, color=f'C{i}')\n",
    "    axes[i].set_title(f\"{conn['name'][:30]}\\nn={conn['n_meas']}\")\n",
    "    axes[i].set_xlabel('Tau (ms)')\n",
    "    axes[i].axvline(conn['tau_mean'], color='red', linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Conexiones subcorticales/límbicas:\")\n",
    "for conn in subcortical_connections[:8]:\n",
    "    print(f\"{conn['name']}: {conn['n_meas']} meas, τ={conn['tau_mean']:.2f}±{conn['tau_std']:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189558f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis completo de distribuciones de retrasos\n",
    "def comprehensive_delay_analysis(data, roi_names, min_measurements=20):\n",
    "    all_connections = []\n",
    "    \n",
    "    for (i, j), measurements in data.items():\n",
    "        if len(measurements) >= min_measurements:\n",
    "            tau_values = np.array([m[2] for m in measurements])\n",
    "            \n",
    "            # Clasificar tipo conexión\n",
    "            roi_i = roi_names.iloc[i]['roi_name'] if i < len(roi_names) else f\"ROI_{i}\"\n",
    "            roi_j = roi_names.iloc[j]['roi_name'] if j < len(roi_names) else f\"ROI_{j}\"\n",
    "            \n",
    "            # Determinar tipo anatómico\n",
    "            cortical_keywords = ['area', 'cortex', 'motor', 'visual', 'auditory', 'somatosensory']\n",
    "            is_cortical_i = any(kw in roi_i.lower() for kw in cortical_keywords)\n",
    "            is_cortical_j = any(kw in roi_j.lower() for kw in cortical_keywords)\n",
    "            \n",
    "            if is_cortical_i and is_cortical_j:\n",
    "                conn_type = 'Cortico-cortical'\n",
    "            elif not is_cortical_i and not is_cortical_j:\n",
    "                conn_type = 'Subcortical'\n",
    "            else:\n",
    "                conn_type = 'Mixed'\n",
    "            \n",
    "            all_connections.append({\n",
    "                'pair': (i, j),\n",
    "                'type': conn_type,\n",
    "                'n_meas': len(measurements),\n",
    "                'tau_mean': tau_values.mean(),\n",
    "                'tau_median': np.median(tau_values),\n",
    "                'tau_std': tau_values.std(),\n",
    "                'tau_min': tau_values.min(),\n",
    "                'tau_max': tau_values.max(),\n",
    "                'tau_skew': pd.Series(tau_values).skew(),\n",
    "                'tau_p25': np.percentile(tau_values, 25),\n",
    "                'tau_p75': np.percentile(tau_values, 75)\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(all_connections)\n",
    "\n",
    "# Ejecutar análisis\n",
    "df_analysis = comprehensive_delay_analysis(data, roi_names)\n",
    "\n",
    "# Estadísticas por tipo de conexión\n",
    "print(\"Retrasos por tipo de conexión:\")\n",
    "summary = df_analysis.groupby('type').agg({\n",
    "    'n_meas': ['count', 'mean'],\n",
    "    'tau_mean': ['mean', 'std'],\n",
    "    'tau_std': 'mean'\n",
    "}).round(3)\n",
    "print(summary)\n",
    "\n",
    "# Visualización comparativa\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for i, conn_type in enumerate(['Cortico-cortical', 'Subcortical', 'Mixed']):\n",
    "    subset = df_analysis[df_analysis['type'] == conn_type]\n",
    "    axes[i].hist(subset['tau_mean'], bins=20, alpha=0.7)\n",
    "    axes[i].set_title(f'{conn_type}\\n(n={len(subset)})')\n",
    "    axes[i].set_xlabel('Tau medio (ms)')\n",
    "    axes[i].axvline(subset['tau_mean'].mean(), color='red', linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTop 10 conexiones por número de mediciones:\")\n",
    "print(df_analysis.nlargest(10, 'n_meas')[['type', 'n_meas', 'tau_mean', 'tau_std']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f5b71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data, roi_names):\n",
    "    cleaned_connections = {}\n",
    "    cleaning_stats = {\n",
    "        'original': len(data),\n",
    "        'nan_removed': 0,\n",
    "        'min_measurements': 0,\n",
    "        'tau_outliers': 0,\n",
    "        'quality_filtered': 0,\n",
    "        'empty_connections': 0,\n",
    "        'final': 0\n",
    "    }\n",
    "    \n",
    "    for (i, j), measurements in data.items():\n",
    "        if not measurements:\n",
    "            cleaning_stats['empty_connections'] += 1\n",
    "            continue\n",
    "        # Convert to array for processing\n",
    "        values_array = np.array(measurements)\n",
    "        \n",
    "        # 1. Remove NaN/inf\n",
    "        valid_mask = np.all(np.isfinite(values_array), axis=1)\n",
    "        if not valid_mask.any():\n",
    "            cleaning_stats['nan_removed'] += 1\n",
    "            continue\n",
    "        values_clean = values_array[valid_mask]\n",
    "        \n",
    "        # 2. Minimum measurements threshold\n",
    "        if len(values_clean) < 50:\n",
    "            cleaning_stats['min_measurements'] += 1\n",
    "            continue\n",
    "            \n",
    "        # 3. Tau outlier removal (2-10ms)\n",
    "        tau_values = values_clean[:, 2]\n",
    "        tau_mask = (tau_values >= 2.0) & (tau_values <= 10.0)\n",
    "        if tau_mask.sum() < 20:  # Need minimum after tau filtering\n",
    "            cleaning_stats['tau_outliers'] += 1\n",
    "            continue\n",
    "        values_filtered = values_clean[tau_mask]\n",
    "        \n",
    "        # 4. Quality control (Val_5 > 0.05)\n",
    "        quality_values = values_filtered[:, 5]\n",
    "        quality_threshold = np.percentile(quality_values, 10)\n",
    "        quality_mask = quality_values > quality_threshold\n",
    "        if quality_mask.sum() < 10:\n",
    "            cleaning_stats['quality_filtered'] += 1\n",
    "            continue\n",
    "        values_final = values_filtered[quality_mask]\n",
    "        \n",
    "        cleaned_connections[(i, j)] = values_final.tolist()\n",
    "        cleaning_stats['final'] += 1\n",
    "    \n",
    "    return cleaned_connections, cleaning_stats\n",
    "\n",
    "# Apply cleaning\n",
    "cleaned_data, stats = clean_data(data, roi_names)\n",
    "\n",
    "print(\"Cleaning results:\")\n",
    "for step, count in stats.items():\n",
    "    print(f\"{step}: {count}\")\n",
    "print(f\"Retention rate: {stats['final']/stats['original']:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39813c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_strong_connections(cleaned_data):\n",
    "   # Calculate mean connectivity metrics from cleaned data\n",
    "   all_val0 = []\n",
    "   all_val1 = []\n",
    "   \n",
    "   for measurements in cleaned_data.values():\n",
    "       values_array = np.array(measurements)\n",
    "       all_val0.extend(values_array[:, 0])\n",
    "       all_val1.extend(values_array[:, 1])\n",
    "   \n",
    "   # Calculate sigma thresholds\n",
    "   val0_mean, val0_std = np.mean(all_val0), np.std(all_val0)\n",
    "   val1_mean, val1_std = np.mean(all_val1), np.std(all_val1)\n",
    "   \n",
    "   val0_threshold = val0_mean + val0_std  # μ + 1σ\n",
    "   val1_threshold = val1_mean + val1_std\n",
    "   \n",
    "   print(f\"Val0: μ={val0_mean:.6f}, σ={val0_std:.6f}, threshold={val0_threshold:.6f}\")\n",
    "   print(f\"Val1: μ={val1_mean:.6f}, σ={val1_std:.6f}, threshold={val1_threshold:.6f}\")\n",
    "   \n",
    "   # Filter strong connections\n",
    "   strong_connections = {}\n",
    "   for (i, j), measurements in cleaned_data.items():\n",
    "       values_array = np.array(measurements)\n",
    "       \n",
    "       # Use mean connectivity per connection\n",
    "       conn_val0_mean = np.mean(values_array[:, 0])\n",
    "       conn_val1_mean = np.mean(values_array[:, 1])\n",
    "       \n",
    "       if conn_val0_mean > val0_threshold or conn_val1_mean > val1_threshold:\n",
    "           strong_connections[(i, j)] = measurements\n",
    "   \n",
    "   print(f\"\\nStrong connections: {len(strong_connections)}/{len(cleaned_data)} ({len(strong_connections)/len(cleaned_data):.1%})\")\n",
    "   \n",
    "   return strong_connections\n",
    "\n",
    "# Apply strength filtering\n",
    "strong_data = select_strong_connections(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58756bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
